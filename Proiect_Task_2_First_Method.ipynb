{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deepfake Image Detection\n",
        "\n",
        "Autori: Bucă Mihnea-Vicențiu; Căpatână Răzvan-Nicolae; Luculescu Teodor\n"
      ],
      "metadata": {
        "id": "5gGoDsd7pzeb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model attribution"
      ],
      "metadata": {
        "id": "3eqU3iaTqdgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part we investigate whether we can identify the generative model that has produced a particular image. We formulate this task as a multiclass classification task, where the input is an image and the output is one of the five classes: “ldm”, “lama”, “pluralistic”, “repaint”, “real”. Experiment with the same methods as for the first task. Report the overall accuracy and the per class accuracy. Display a TSNE plot of the features color coded by the five classes."
      ],
      "metadata": {
        "id": "pnKplpbkqlir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "\n",
        "The dataset can be downloaded from [here](https://drive.google.com/file/d/1NfLX9bZtOY8dO_yj3cU7pEHGmqItqjg2/view). It contains real images from the CelebAHQ dataset and locally manipulated images produced by four generators: [LDM](https://github.com/CompVis/latent-diffusion), [Pluralistic](https://github.com/lyndonzheng/Pluralistic-Inpainting), [LAMA](https://github.com/advimman/lama), [Repaint](https://github.com/andreas128/RePaint). You can read more about how this dataset was produced in Section 3.3 of the following paper:"
      ],
      "metadata": {
        "id": "HFC54RWeq9qP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "F-5LO_LYrBYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# path to the zip file\n",
        "zip_file_path = 'drive/MyDrive/Proiect DeepLearning/DeepFMI_local_data.zip'\n",
        "\n",
        "# the paths to the datasets within the zip file\n",
        "dataset_paths = [\n",
        "    'FMI_local_data/celebhq_real_data',\n",
        "    'FMI_local_data/lama',\n",
        "    'FMI_local_data/ldm',\n",
        "    'FMI_local_data/pluralistic',\n",
        "    'FMI_local_data/repaint'\n",
        "]\n",
        "\n",
        "# create a ZipFile object\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Iterate through the dataset paths\n",
        "    for dataset_path in dataset_paths:\n",
        "         zip_ref.extractall(members=[\n",
        "            name for name in zip_ref.namelist()\n",
        "            if name.startswith(dataset_path)\n",
        "        ], path='/content/')  # Extract to the '/content/' directory\n"
      ],
      "metadata": {
        "id": "8gELgsDgsFaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Models"
      ],
      "metadata": {
        "id": "B7B2HsTusL6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# important libraries\n",
        "import torch\n",
        "import glob\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import timm\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "from sklearn.metrics import average_precision_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "from tqdm import tqdm  # for progress bar"
      ],
      "metadata": {
        "id": "QTpnNE2IsO5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepFakeDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Takes a folder of real images and a list of folders of fake images and assigns labels (0 for real, 1/2/3/4 for each fake image).\n",
        "    0 = real\n",
        "    1 = lama\n",
        "    2 = ldm\n",
        "    3 = repaint\n",
        "    4 = pluralistic\n",
        "    \"\"\"\n",
        "    def __init__(self, real_folder: str, fake_folders: list[str], transform=None):\n",
        "        # grab all .png under each\n",
        "        self.real_paths = sorted(glob.glob(os.path.join(real_folder, '*.png')))\n",
        "        self.samples = [(p, 0) for p in self.real_paths]\n",
        "\n",
        "        self.fake_paths = []\n",
        "        for label, fake_folder in enumerate(fake_folders):\n",
        "            current_fake_paths = sorted(glob.glob(os.path.join(fake_folder, '*.png')))\n",
        "            self.fake_paths += current_fake_paths\n",
        "            self.samples += [(p, label + 1) for p in current_fake_paths]\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label"
      ],
      "metadata": {
        "id": "JSzpi4fXsRGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model_dataloaders(\n",
        "    root_dir: str,            # contains subfolders: lama/, ldm/, repaint/, pluralistic/\n",
        "    real_root: str,           # path to celebhq_real_data\n",
        "    model_names: list[str],   # ['lama','ldm','repaint','pluralistic']\n",
        "    splits: list[str] = ('train','valid','test'),\n",
        "    batch_size: int = 16,\n",
        "    img_size: int = 256,\n",
        "    num_workers: int = 2\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns a dict:\n",
        "      { split: DataLoader, … }\n",
        "      Each loader mixes real vs models' fake images.\n",
        "    \"\"\"\n",
        "\n",
        "    # strong augmentation for train\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(img_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914,0.4822,0.4465), (0.2023,0.1994,0.2010))\n",
        "    ])\n",
        "\n",
        "    # weak augmentation for val/test\n",
        "    test_tf = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914,0.4822,0.4465), (0.2023,0.1994,0.2010))\n",
        "    ])\n",
        "\n",
        "    dataloaders = {}\n",
        "    for split in splits:\n",
        "        real_folder = os.path.join(real_root, split)\n",
        "        fake_folders = [os.path.join(root_dir, model_name, split) for model_name in model_names]\n",
        "\n",
        "        tf = train_tf if split=='train' else test_tf\n",
        "        ds = DeepFakeDataset(real_folder, fake_folders, transform=tf)\n",
        "\n",
        "        dataloaders[split] = DataLoader(\n",
        "            ds,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=(split=='train'),\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "    return dataloaders"
      ],
      "metadata": {
        "id": "KV4Pm1gqxiYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root = \"/content/FMI_local_data\"\n",
        "deepfake_models = [\"lama\", \"ldm\", \"repaint\", \"pluralistic\"]\n",
        "loaders = make_model_dataloaders(\n",
        "    root_dir=root,\n",
        "    real_root=os.path.join(root, \"celebhq_real_data\"),\n",
        "    model_names=deepfake_models,\n",
        "    splits=['train', 'valid', 'test'],\n",
        "    batch_size=16,\n",
        "    img_size=256,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "# test\n",
        "train_loader = loaders['train']\n",
        "print(f\"train batches: {len(train_loader)}\")"
      ],
      "metadata": {
        "id": "2XFsDOS82-IR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_timm_scratch_one_epoch(\n",
        "    dataloaders: dict,\n",
        "    model_name: str = 'xception41',\n",
        "    num_classes: int = 5,\n",
        "    lr: float = 1e-3,\n",
        "    device: str = None\n",
        "):\n",
        "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    results = {}\n",
        "\n",
        "    label_to_model = {\n",
        "        0: 'real',\n",
        "        1: 'lama',\n",
        "        2: 'ldm',\n",
        "        3: 'repaint',\n",
        "        4: 'pluralistic'\n",
        "    }\n",
        "\n",
        "    print(f\"\\n=== Training {model_name} from scratch (1 epoch) ===\")\n",
        "\n",
        "    # 1) instantiate model without pretrained weights\n",
        "    model = timm.create_model(model_name, pretrained=False, num_classes=num_classes)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # 2) loss & optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # 3) Single training epoch\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    train_label_cnts = [0] * num_classes\n",
        "    train_accs = [0] * num_classes\n",
        "\n",
        "    for imgs, labels in dataloaders['train']:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs)             # raw logits\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "        for pred, label in zip(preds, labels):\n",
        "            train_label_cnts[label] += 1\n",
        "            if pred == label:\n",
        "                train_accs[label] += 1\n",
        "\n",
        "\n",
        "    avg_loss = running_loss / len(dataloaders['train'].dataset)\n",
        "    print(f\"  Train 1-epoch loss: {avg_loss:.4f}\")\n",
        "\n",
        "    train_acc = sum(train_accs) / sum(train_label_cnts)\n",
        "    print(f\"  Train accuracy: {train_acc:.4%}\")\n",
        "\n",
        "    for label in range(num_classes):\n",
        "        train_acc = train_accs[label] / train_label_cnts[label]\n",
        "        print(f\"    Train accuracy for class {label_to_model[label]}: {train_acc:.4%}\")\n",
        "\n",
        "    # 4) Validation\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "\n",
        "    valid_label_cnts = [0] * num_classes\n",
        "    valid_accs = [0] * num_classes\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in dataloaders['valid']:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            preds = model(imgs).argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            for pred, label in zip(preds, labels):\n",
        "                valid_label_cnts[label] += 1\n",
        "                if pred == label:\n",
        "                    valid_accs[label] += 1\n",
        "\n",
        "\n",
        "    valid_acc = correct / total\n",
        "    print(f\"  Valid accuracy: {valid_acc:.4%}\")\n",
        "\n",
        "    for label in range(num_classes):\n",
        "        valid_acc = valid_accs[label] / valid_label_cnts[label]\n",
        "        print(f\"    Valid accuracy for class {label_to_model[label]}: {valid_acc:.4%}\")\n",
        "\n",
        "\n",
        "    results = {\n",
        "        'model': model,\n",
        "        'train_loss': avg_loss,\n",
        "        'train_acc': train_acc,\n",
        "        'valid_acc': valid_acc\n",
        "    }\n",
        "    for label in range(num_classes):\n",
        "        results[f'train_acc_{label_to_model[label]}'] = train_accs[label] / train_label_cnts[label]\n",
        "        results[f'valid_acc_{label_to_model[label]}'] = valid_accs[label] / valid_label_cnts[label]\n",
        "\n",
        "    # clear GPU memory\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "0NHektWJ3Jn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = train_timm_scratch_one_epoch(\n",
        "    dataloaders=loaders,\n",
        "    model_name='xception41',\n",
        "    num_classes=5,\n",
        "    lr=1e-3\n",
        ")"
      ],
      "metadata": {
        "id": "SpgtsO_I3_o8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save models in drive/MyDrive/Proiect DeepLearning/Task-2/First-Method\n",
        "save_path = 'drive/MyDrive/Proiect DeepLearning/Task-2/First-Method/results.pth'\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)  # Ensure the directory exists\n",
        "torch.save(results, save_path)\n",
        "print(f\"Results saved to: {save_path}\")"
      ],
      "metadata": {
        "id": "ljb8EUmy4JTn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}