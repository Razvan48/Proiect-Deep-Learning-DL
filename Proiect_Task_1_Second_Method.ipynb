{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deepfake Image Detection\n",
        "\n",
        "Autori: Bucă Mihnea-Vicențiu; Căpatână Răzvan-Nicolae; Luculescu Teodor\n"
      ],
      "metadata": {
        "id": "Ce_asoA0KSYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-generator deepfake detection"
      ],
      "metadata": {
        "id": "RyYOgdXHKYp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to evaluate the generalization capabilities of deepfake detection methods: how well detectors work when tested on images produced by other generators than those seen at training. For this, we will train on images coming from one generator and test on images coming from other generators. We will compare at least three different methods."
      ],
      "metadata": {
        "id": "eSkvV7glKaFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First method:\n",
        "- We will use an image classification architecture, mainly ResNet, that will be trained from scratch."
      ],
      "metadata": {
        "id": "D2GMmv7-Kb3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second method:\n",
        "- The same architecture as above, but this time initialized with pre-trained weights. The weights will be obtained by supervised learning (image classification on ImageNet).\n"
      ],
      "metadata": {
        "id": "sAVH4aNBKd4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Third method:\n",
        "- Large pretrained self-supervised representations followed by a linear classifier. In this case, only the linear classifier is trained; the representations are extracted with a frozen model. [Ojha et al., (2023)](https://github.com/WisconsinAIVision/UniversalFakeDetect) have used this approach in the context of deepfake detection, but differently from us, they have applied it to general fully-generated images. We will train two models, one using CLIP and one using SAM self-supervised representations and compare the results obtained.\n"
      ],
      "metadata": {
        "id": "ww1oPl4kKgJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each method, fill in a table with the average precisions."
      ],
      "metadata": {
        "id": "1167178OKh79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will use the **Second method**"
      ],
      "metadata": {
        "id": "nHhztFLgKjrX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "\n",
        "The dataset can be downloaded from [here](https://drive.google.com/file/d/1NfLX9bZtOY8dO_yj3cU7pEHGmqItqjg2/view). It contains real images from the CelebAHQ dataset and locally manipulated images produced by four generators: [LDM](https://github.com/CompVis/latent-diffusion), [Pluralistic](https://github.com/lyndonzheng/Pluralistic-Inpainting), [LAMA](https://github.com/advimman/lama), [Repaint](https://github.com/andreas128/RePaint). You can read more about how this dataset was produced in Section 3.3 of the following paper:"
      ],
      "metadata": {
        "id": "R9mY50-GKl4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8Q3h1fBKZr4",
        "outputId": "2a684493-ac49-4366-adbf-cb6ba0afbefb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will extract the data for each model"
      ],
      "metadata": {
        "id": "xeBT6J3RPOQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# path to the zip file\n",
        "zip_file_path = 'drive/MyDrive/Proiect DeepLearning/DeepFMI_local_data.zip'\n",
        "\n",
        "# the paths to the datasets within the zip file\n",
        "dataset_paths = [\n",
        "    'FMI_local_data/celebhq_real_data',\n",
        "    'FMI_local_data/lama',\n",
        "    'FMI_local_data/ldm',\n",
        "    'FMI_local_data/pluralistic',\n",
        "    'FMI_local_data/repaint'\n",
        "]\n",
        "\n",
        "# create a ZipFile object\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Iterate through the dataset paths\n",
        "    for dataset_path in dataset_paths:\n",
        "         zip_ref.extractall(members=[\n",
        "            name for name in zip_ref.namelist()\n",
        "            if name.startswith(dataset_path)\n",
        "        ], path='/content/')  # Extract to the '/content/' directory\n"
      ],
      "metadata": {
        "id": "8feKo4vDPLc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Models"
      ],
      "metadata": {
        "id": "shpp0hdnPsEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "!pip install sam"
      ],
      "metadata": {
        "id": "hJtrAyagyVh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# important libraries\n",
        "import torch\n",
        "import glob\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import timm\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "from sklearn.metrics import average_precision_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "from PIL import Image\n",
        "from tqdm import tqdm  # for progress bar"
      ],
      "metadata": {
        "id": "g9fKg3BCPxvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepFakeDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Takes two folders (real vs fake) and assigns labels 0 / 1.\n",
        "    \"\"\"\n",
        "    def __init__(self, real_folder: str, fake_folder: str, transform=None):\n",
        "        # grab all .png under each\n",
        "        self.real_paths = sorted(glob.glob(os.path.join(real_folder, '*.png')))\n",
        "        self.fake_paths = sorted(glob.glob(os.path.join(fake_folder, '*.png')))\n",
        "\n",
        "        # create a single list of (path, label)\n",
        "        # real = 0, fake = 1\n",
        "        self.samples = (\n",
        "            [(p, 0) for p in self.real_paths] +\n",
        "            [(p, 1) for p in self.fake_paths]\n",
        "        )\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label"
      ],
      "metadata": {
        "id": "VjYtE4fNm94s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model_dataloaders(\n",
        "    root_dir: str,            # contains subfolders: lama/, ldm/, repaint/, pluralistic/\n",
        "    real_root: str,           # path to celebhq_real_data\n",
        "    model_names: list[str],   # ['lama','ldm','repaint','pluralistic']\n",
        "    splits: list[str] = ('train','valid','test'),\n",
        "    batch_size: int = 16,\n",
        "    img_size: int = 256,\n",
        "    num_workers: int = 2\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns a dict:\n",
        "      { model_name: { split: DataLoader, … }, … }\n",
        "      Each loader mixes real vs that model's fake images.\n",
        "    \"\"\"\n",
        "\n",
        "    # strong augmentation for train\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(img_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914,0.4822,0.4465), (0.2023,0.1994,0.2010))\n",
        "    ])\n",
        "\n",
        "    # weak augmentation for val/test\n",
        "    test_tf = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914,0.4822,0.4465), (0.2023,0.1994,0.2010))\n",
        "    ])\n",
        "\n",
        "    dataloaders = {}\n",
        "    for model_name in model_names:\n",
        "            dataloaders[model_name] = {}\n",
        "            for split in splits:\n",
        "                real_folder = os.path.join(real_root, split)\n",
        "                fake_folder = os.path.join(root_dir, model_name, split)\n",
        "\n",
        "                tf = train_tf if split=='train' else test_tf\n",
        "                ds = DeepFakeDataset(real_folder, fake_folder, transform=tf)\n",
        "\n",
        "                dataloaders[model_name][split] = DataLoader(\n",
        "                    ds,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=(split=='train'),\n",
        "                    num_workers=num_workers,\n",
        "                    pin_memory=True\n",
        "                )\n",
        "\n",
        "    return dataloaders"
      ],
      "metadata": {
        "id": "8DghBZb6P1dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root = \"/content/FMI_local_data\"\n",
        "deepfake_models = [\"lama\", \"ldm\", \"repaint\", \"pluralistic\"]\n",
        "loaders = make_model_dataloaders(\n",
        "    root_dir=root,\n",
        "    real_root=os.path.join(root, \"celebhq_real_data\"),\n",
        "    model_names=deepfake_models,\n",
        "    splits=['train', 'valid', 'test'],\n",
        "    batch_size=16,\n",
        "    img_size=256,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "# test\n",
        "ldm_train_loader = loaders['ldm']['train']\n",
        "print(f\"ldm train batches: {len(ldm_train_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7p0yIBqZP3Qk",
        "outputId": "b66bf3bb-2998-4ad4-8741-eb5641e62a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ldm train batches: 1125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_timm(\n",
        "    dataloaders,         # dict: { name: {'train','valid','test'} DataLoaders }\n",
        "    num_classes: int = 2,\n",
        "    num_epochs: int = 2,\n",
        "    lr_head: float = 1e-3,\n",
        "    lr_ft: float = 1e-4,\n",
        "    weight_decay: float = 1e-5,\n",
        "    freeze_epochs: int = 5,\n",
        "    device: str = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Fine-tune Xception41 pretrained on ImageNet-1K (supervised)\n",
        "    - Phase 1: freeze backbone, train only the new classifier head\n",
        "    - Phase 2: unfreeze entire network and fine-tune\n",
        "    \"\"\"\n",
        "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    results = {}\n",
        "\n",
        "    for name, splits in dataloaders.items():\n",
        "        print(f\"\\n=== [Xception-Imagenet] Training on '{name}' ===\")\n",
        "\n",
        "        # 1) Load Xception41 with supervised ImageNet‐1K weights\n",
        "        model = timm.create_model(\n",
        "            'xception41',\n",
        "            pretrained='imagenet',\n",
        "            num_classes=num_classes\n",
        "        ).to(device)\n",
        "\n",
        "        # 2) Phase 1: freeze all layers except the classifier head\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = False\n",
        "        # timm’s get_classifier() returns the final Linear\n",
        "        for p in model.get_classifier().parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "        optimizer = optim.Adam(\n",
        "            model.get_classifier().parameters(),\n",
        "            lr=lr_head,\n",
        "            weight_decay=weight_decay\n",
        "        )\n",
        "        scheduler = optim.lr_scheduler.StepLR(\n",
        "            optimizer, step_size=freeze_epochs, gamma=0.1\n",
        "        )\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Head-only training\n",
        "        for epoch in range(1, freeze_epochs + 1):\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            for imgs, labels in splits['train']:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(imgs)             # raw logits\n",
        "                loss = criterion(logits, labels) # CE on logits\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item() * imgs.size(0)\n",
        "            scheduler.step()\n",
        "            avg = running_loss / len(splits['train'].dataset)\n",
        "            print(f\" [Head] Epoch {epoch}/{freeze_epochs} — loss: {avg:.4f}\")\n",
        "\n",
        "        # 3) Phase 2: unfreeze everything, fine-tune\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "        optimizer = optim.Adam(\n",
        "            model.parameters(),\n",
        "            lr=lr_ft,\n",
        "            weight_decay=weight_decay\n",
        "        )\n",
        "        scheduler = optim.lr_scheduler.StepLR(\n",
        "            optimizer, step_size=5, gamma=0.1\n",
        "        )\n",
        "\n",
        "        for epoch in range(freeze_epochs + 1, num_epochs + 1):\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            for imgs, labels in splits['train']:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(imgs)\n",
        "                loss = criterion(logits, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item() * imgs.size(0)\n",
        "            scheduler.step()\n",
        "            avg = running_loss / len(splits['train'].dataset)\n",
        "            print(f\" [Fine-tune] Epoch {epoch}/{num_epochs} — loss: {avg:.4f}\")\n",
        "\n",
        "        # 4) Validation\n",
        "        model.eval()\n",
        "        correct = total = 0\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in splits['valid']:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                preds = model(imgs).argmax(dim=1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "        valid_acc = correct / total\n",
        "        print(f\" ▶ valid acc {valid_acc:.4%}\")\n",
        "\n",
        "\n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'valid_acc': valid_acc,\n",
        "        }\n",
        "\n",
        "        # clear GPU memory\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "ev1J4Tgg3Ude"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = train_timm(\n",
        "    dataloaders=loaders,\n",
        "    num_classes = 2,\n",
        "    num_epochs = 2,\n",
        "    lr_head = 1e-3,\n",
        "    lr_ft = 1e-4,\n",
        "    weight_decay = 1e-5,\n",
        "    freeze_epochs = 1,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZXE-6U6Qixh",
        "outputId": "99d0f36e-0b3a-4767-b16d-a482bbba0753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== [Xception-Imagenet] Training on 'lama' ===\n",
            " [Head] Epoch 1/1 — loss: 0.4419\n",
            " [Fine-tune] Epoch 2/2 — loss: 0.0921\n",
            " ▶ valid acc 98.5000%\n",
            "\n",
            "=== [Xception-Imagenet] Training on 'ldm' ===\n",
            " [Head] Epoch 1/1 — loss: 0.5636\n",
            " [Fine-tune] Epoch 2/2 — loss: 0.1161\n",
            " ▶ valid acc 95.5000%\n",
            "\n",
            "=== [Xception-Imagenet] Training on 'repaint' ===\n",
            " [Head] Epoch 1/1 — loss: 0.7082\n",
            " [Fine-tune] Epoch 2/2 — loss: 0.6140\n",
            " ▶ valid acc 74.3889%\n",
            "\n",
            "=== [Xception-Imagenet] Training on 'pluralistic' ===\n",
            " [Head] Epoch 1/1 — loss: 0.6563\n",
            " [Fine-tune] Epoch 2/2 — loss: 0.3781\n",
            " ▶ valid acc 77.9444%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# save models in drive/MyDrive/Proiect DeepLearning/Second-Method\n",
        "save_path = 'drive/MyDrive/Proiect DeepLearning/Second-Method/results.pth'\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)  # Ensure the directory exists\n",
        "torch.save(results, save_path)\n",
        "print(f\"Results saved to: {save_path}\")"
      ],
      "metadata": {
        "id": "zgGgx7sTUc6u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "0b3913e5-bc89-44f2-ce52-58d61a5fcedc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-213859324878>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# save models in drive/MyDrive/Proiect DeepLearning/Second-Method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'drive/MyDrive/Proiect DeepLearning/Second-Method/results.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure the directory exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Results saved to: {save_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cross_table_ap(results: dict, dataloaders: dict, device=None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a cross-table with average precision score (macro-averaged).\n",
        "    Assumes classification with multiple classes.\n",
        "    \"\"\"\n",
        "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    gens = list(results.keys())\n",
        "    table = pd.DataFrame(index=gens, columns=gens, dtype=float)\n",
        "\n",
        "    for train_gen, result_dict in results.items():\n",
        "        model = result_dict['model']\n",
        "        model.to(device).eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for test_gen in gens:\n",
        "                all_probs = []\n",
        "                all_targets = []\n",
        "                for imgs, labels in dataloaders[test_gen]['test']:\n",
        "                    imgs = imgs.to(device)\n",
        "                    logits = model(imgs)  # raw outputs\n",
        "                    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "                    all_probs.append(probs)\n",
        "                    all_targets.append(labels.cpu().numpy())\n",
        "\n",
        "                all_probs = np.concatenate(all_probs, axis=0)\n",
        "                all_targets = np.concatenate(all_targets, axis=0)\n",
        "\n",
        "                # Convert labels to one-hot\n",
        "                num_classes = all_probs.shape[1]\n",
        "                targets_onehot = np.eye(num_classes)[all_targets]\n",
        "\n",
        "                ap_score = average_precision_score(targets_onehot, all_probs, average='macro')\n",
        "                table.loc[test_gen, train_gen] = ap_score\n",
        "\n",
        "    return table"
      ],
      "metadata": {
        "id": "JEvjEZYJKqp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.serialization.add_safe_globals([timm.models.xception_aligned.XceptionAligned])\n",
        "# tmp = torch.load('drive/MyDrive/Proiect DeepLearning/Second-Method/results.pth', weights_only=False)\n",
        "df = build_cross_table_ap(results, loaders)\n",
        "print(\"\\nCross-Generator Accuracy Table:\\n\")\n",
        "display(Markdown(df.to_markdown(floatfmt=\".3f\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "8ww45ExYPJGd",
        "outputId": "7b49e468-8d8e-4c95-c28e-4c83ca381fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-Generator Accuracy Table:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "|             |   lama |   ldm |   repaint |   pluralistic |\n|:------------|-------:|------:|----------:|--------------:|\n| lama        |  0.998 | 0.310 |     0.445 |         0.626 |\n| ldm         |  0.410 | 0.993 |     0.906 |         0.460 |\n| repaint     |  0.486 | 0.636 |     0.800 |         0.531 |\n| pluralistic |  0.613 | 0.421 |     0.586 |         0.799 |"
          },
          "metadata": {}
        }
      ]
    }
  ]
}