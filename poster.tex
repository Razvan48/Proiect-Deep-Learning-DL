% Unofficial University of Oxford Poster Template
% https://github.com/andiac/gemini-cam
% a fork of https://github.com/anishathalye/gemini

\documentclass[final]{beamer}

% ====================
% Packages
% ====================

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[size=custom,orientation=portrait,width=120,height=72,scale=1.0]{beamerposter}
\usetheme{gemini}
\usecolortheme{ox}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.14}
\usepackage{anyfontsize}
\usepackage[absolute,overlay]{textpos}

\usepackage[sorting=none,backend=biber]{biblatex}
\addbibresource{poster.bib} 

% ====================
% Lengths
% ====================

% If you have N columns, choose \sepwidth and \colwidth such that
% (N+1)*\sepwidth + N*\colwidth = \paperwidth
\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.010\paperwidth}
\setlength{\colwidth}{0.185\paperwidth}


\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}

% ====================
% Title
% ====================

\title{Deepfake Image Detection}

\author{Bucă Mihnea-Vicențiu \and Căpățînă Răzvan-Nicolae \and Luculescu Teodor}

\institute[shortinst]{University of Bucharest, Faculty of Mathematics and Informatics}

% ====================
% Footer (optional)
% ====================

\footercontent{
  Deepfake Image Detection \hfill
  Deep Learning, Bitdefender \hfill
  Mentors: eoneata@bitdefender.com, ssmeu@bitdefender.com
}
% (can be left out to remove footer)

% ====================
% Logo (optional)
% ====================
% Refer to https://github.com/k4rtik/uchicago-poster
% logo: https://communications.admin.ox.ac.uk/communications-resources/visual-identity/identity-guidelines/the-oxford-logo
% use this to include logos on the left and/or right side of the header:

\logoright{\includegraphics[height=4cm]{logos/fmi.png}}
\logoleft{\includegraphics[height=4cm]{logos/ub.jpg}}

% ====================
% Body
% ====================

\begin{document}



\begin{frame}[t]
\begin{columns}[t]
\separatorcolumn
\begin{column}{\colwidth}
  \begin{block}{Objectives}
    There are two main parts to this project:
    \begin{enumerate}
      \item Analysis of cross-generator deepfake detection using three different approaches
      \item Model attribution
    \end{enumerate}
  \end{block}
  
  \begin{block}{Dataset}
  The dataset contains real images from the \textit{CelebAHQ} and locally manipulated images produced by four generators: \textit{LDM, Pluralistic, LAMA, Repaint}. 
  You can read more about how this dataset was produced in \textbf{Section 3.3} of the following paper: \cite{2311.04584}
  \end{block}

  \begin{block}{Cross-Generator Deepfake Detection}
    \begin{block}{First Method}
      This method trains an \textit{xception41} \cite{Timmxception41tf} model from scratch (from \textit{timm} \cite{huggingfaceTimm}) on multiple datasets for one epoch each, using \textbf{Adam} optimizer and \textbf{cross-entropy loss}. 

      \begin{table}
        \centering
        \begin{tabular}{l r r r r}
          \toprule
          \textbf{Tested On \textbackslash Trained On} & \textbf{lama} & \textbf{ldm} & \textbf{repaint} & \textbf{pluralistic} \\
          \midrule
          lama       & 0.641 & 0.493 & 0.536 & 0.517 \\
          ldm        & 0.569 & 0.496 & 0.511 & 0.536 \\
          repaint    & 0.572 & 0.487 & 0.517 & 0.531 \\
          pluralistic & 0.667 & 0.491 & 0.580 & 0.700 \\
          \bottomrule
        \end{tabular}
        \caption{Performance of model \textit{xception41} (not pretrained); trained and tested on the provided datasets.}
      \end{table}
    \end{block}

    \begin{figure}
      \centering
      \includegraphics[width=\textwidth]{imgs/task1_method1.png}
      \caption{Example of \textbf{GRAD-CAM} on the testsets, model is trained on pluralistic.}
    \end{figure}
\end{block}
\begin{alertblock}{Method Performance}
  \begin{itemize}
    \item \textbf{Cross-domain} generalization is limited; 
    \item \textit{Pluralistic} performs best overall;
    \item \textit{LDM} and \textit{Repaint} are harder to detect;
    \item \textbf{Grad-CAM} activations often focus on background or peripheral areas;
  \end{itemize}
\end{alertblock}
\end{column}
\separatorcolumn
\begin{column}{\colwidth}

  \begin{block}{The Second Method}
    The second method uses the same \textit{xception41} architecture but initializes the model with pretrained weights (\textit{ImageNet}, via the \textit{timm} library), obtained in a self-supervised manner.
    \begin{table}[h]
      \centering
      \begin{tabular}{lcccc}
        \toprule
        \textbf{Tested On \textbackslash Trained On} & \textbf{lama} & \textbf{ldm} & \textbf{repaint} & \textbf{pluralistic} \\
        \midrule
        lama         & 0.998 & 0.310 & 0.445 & 0.626 \\
        ldm          & 0.410 & 0.993 & 0.906 & 0.460 \\
        repaint      & 0.486 & 0.636 & 0.800 & 0.531 \\
        pluralistic  & 0.613 & 0.421 & 0.586 & 0.790 \\
        \bottomrule
      \end{tabular}
      \caption{Accuracy of pretrained XceptionNet (xception41).}
    \end{table}   
    
      \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{imgs/task1_method2.png}
        \caption{Example of \textbf{GRAD-CAM} on the testsets, model is trained on pluralistic.}
      \end{figure}
\end{block}

\begin{alertblock}{Method Performance}
  \begin{itemize}
    \item Strong memorization of known artifacts.
    \item Still struggles with unfamiliar artifact distributions.
    \item \textit{LDM} and \textit{Repaint} is the best cross-domain generalization.
    \item Successful detections focus on localized, high-contrast regions, but in many failed cases, activations are diffuse or absent—implying the model overlooks subtle inpainting traces.
  \end{itemize}
\end{alertblock}
\begin{block}{Third Method}
   The third approach uses large self-supervised models (\textit{CLIP} and \textit{SAM}) as frozen feature extractors, training only a linear classifier on top. Unlike \cite{Ojhae}, who targeted fully-generated images, we focus on detecting inpainted regions via linear probing.
   \begin{table}[h]
  \centering
  \begin{tabular}{l r r r r}
    \toprule
    \textbf{Tested On \textbackslash Trained On} & \textbf{lama} & \textbf{ldm} & \textbf{repaint} & \textbf{pluralistic} \\
    \midrule
    lama         & 1.000 & 0.535 & 0.522 & 0.675 \\
    ldm          & 0.600 & 1.000 & 0.901 & 0.977 \\
    repaint      & 0.511 & 0.620 & 0.723 & 0.615 \\
    pluralistic  & 0.765 & 0.951 & 0.880 & 0.992 \\
    \bottomrule
  \end{tabular}
  \caption{Cross-domain accuracy matrix using \textit{CLIP} features and a linear classifier.}
\end{table}
\end{block}
\end{column}\separatorcolumn\begin{column}{\colwidth}
\begin{block}{The Third Method}
\begin{table}[h]
  \centering
  \begin{tabular}{l r r r r}
    \toprule
    \textbf{Tested On \textbackslash Trained On} & \textbf{lama} & \textbf{ldm} & \textbf{repaint} & \textbf{pluralistic} \\
    \midrule
    lama         & 0.870 & 0.420 & 0.514 & 0.588 \\
    ldm          & 0.349 & 0.741 & 0.552 & 0.561 \\
    repaint      & 0.536 & 0.543 & 0.547 & 0.575 \\
    pluralistic  & 0.647 & 0.527 & 0.543 & 0.627 \\
    \bottomrule
  \end{tabular}
  \caption{Cross-domain accuracy matrix using \textit{SAM} features and a linear classifier.}
\end{table}
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{imgs/task1_method3.png}
  \caption{Example of \textbf{GRAD-CAM} on the testsets, of all the models trained with \textit{CLIP}.}
\end{figure}
\end{block}
\begin{alertblock}{Method Performance}
  \begin{itemize}
    \item \textit{CLIP} outperforms \textit{SAM} in cross-domain generalization.
    \item \textit{CLIP} is more robust to domain shifts.
    \item Each detector is most confident catching fakes created by its own algorithm.
    \item Diffusion-based models can flag each other's edits and manage repaint reasonably well.
    \item The \textit{ldm} variant generalizes best across different inpainting styles.
    \item \textbf{Grad-CAM} zeroes in on manipulated regions when the detector is right and spreads out when it struggles.
  \end{itemize}
\end{alertblock}
\end{column}
\separatorcolumn
\begin{column}{\colwidth}
  \begin{block}{Model attribution.}
    \begin{figure}
      \centering
      \includegraphics[width=0.45\textwidth]{imgs/task2_method1.png}
      \caption{t-SNE plot of the xception41 (not pretrained) model tested on the datasets.
      \textbf{overall:} 45.04$\%$; real: 54.33$\%$; lama: 73.44$\%$, ldm: 83.11$\%$; repaint: 1.22$\%$; pluralistic: 13.11$\%$}
    \end{figure}
    \begin{figure}
      \centering
      \includegraphics[width=0.45\textwidth]{imgs/task2_method2.png}
      \caption{t-SNE plot of the xception41 (pretrained) model tested on the datasets.
      \textbf{overall:} 80.71$\%$; real: 85.22$\%$; lama: 98.44$\%$, ldm: 100$\%$; repaint: 66.88$\%$; pluralistic: 53.00$\%$}
    \end{figure}
    \begin{figure}
      \centering
      \includegraphics[width=0.45\textwidth]{imgs/task2_method3_1.png}
      \caption{t-SNE plot of the CLIP model tested on the datasets.
      \textbf{overall:} 80.24$\%$; real: 68.55$\%$; lama: 99.22$\%$, ldm: 88.55$\%$; repaint: 51.77$\%$; pluralistic: 93.11$\%$}
    \end{figure}
    \begin{figure}
      \centering
      \includegraphics[width=0.45\textwidth]{imgs/task2_method3_2.png}
      \caption{t-SNE plot of the SAM model tested on the datasets.
      \textbf{overall:}  46.20$\%$; real: 17.77$\%$; lama: 92.66$\%$, ldm: 78.55$\%$; repaint: 21.44$\%$; pluralistic: 20.55$\%$}
    \end{figure}
  \end{block}
\end{column}
\separatorcolumn
\begin{column}{\colwidth}

  \begin{alertblock}{Conclusions}
    \begin{itemize}
    \item Scratch-trained model works well on some generators, poorly on others;
    \item Pretrained model improves detection, especially for LAMA and LDM;
    \item Repaint and Pluralistic are still harder to detect;
    \item Pretraining helps with real/fake separation, but domain gaps remain;
    \end{itemize}
  \end{alertblock}
  \begin{block}{References}
    \footnotesize
    \nocite{*}
    \printbibliography
  \end{block}

\end{column}

\separatorcolumn
\end{columns}
% \begin{textblock*}{\paperwidth}(20cm,40cm) % adjust vertical position (Y) as needed
%   \begin{block}{References}
%     \centering
%     \footnotesize
%     \nocite{*}
%     \printbibliography
%   \end{block}
% \end{textblock*}
\end{frame}

\end{document}
