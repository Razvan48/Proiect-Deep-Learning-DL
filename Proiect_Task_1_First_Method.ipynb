{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deepfake Image Detection\n",
        "\n",
        "Autori: Bucă Mihnea-Vicențiu; Căpatână Răzvan-Nicolae; Luculescu Teodor\n"
      ],
      "metadata": {
        "id": "dy7ECiAYy1M3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-generator deepfake detection"
      ],
      "metadata": {
        "id": "ySfDCNrrzUQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to evaluate the generalization capabilities of deepfake detection methods: how well detectors work when tested on images produced by other generators than those seen at training. For this, we will train on images coming from one generator and test on images coming from other generators. We will compare at least three different methods."
      ],
      "metadata": {
        "id": "iW7bPwICzf6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First method:\n",
        "- We will use an image classification architecture, mainly ResNet, that will be trained from scratch.\n"
      ],
      "metadata": {
        "id": "d4FJbOIRzsoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second method:\n",
        "- The same architecture as above, but this time initialized with pre-trained weights. The weights will be obtained by supervised learning (image classification on ImageNet).\n"
      ],
      "metadata": {
        "id": "aMs65JNZ0A6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Third method:\n",
        "- Large pretrained self-supervised representations followed by a linear classifier. In this case, only the linear classifier is trained; the representations are extracted with a frozen model. [Ojha et al., (2023)](https://github.com/WisconsinAIVision/UniversalFakeDetect) have used this approach in the context of deepfake detection, but differently from us, they have applied it to general fully-generated images. We will train two models, one using CLIP and one using SAM self-supervised representations and compare the results obtained.\n"
      ],
      "metadata": {
        "id": "6suqqtNa0RmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each method, fill in a table with the average precisions."
      ],
      "metadata": {
        "id": "3L3q1IoC09Ui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will use the **First method**"
      ],
      "metadata": {
        "id": "vZu_vLF72FMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "\n",
        "The dataset can be downloaded from [here](https://drive.google.com/file/d/1NfLX9bZtOY8dO_yj3cU7pEHGmqItqjg2/view). It contains real images from the CelebAHQ dataset and locally manipulated images produced by four generators: [LDM](https://github.com/CompVis/latent-diffusion), [Pluralistic](https://github.com/lyndonzheng/Pluralistic-Inpainting), [LAMA](https://github.com/advimman/lama), [Repaint](https://github.com/andreas128/RePaint). You can read more about how this dataset was produced in Section 3.3 of the following paper:"
      ],
      "metadata": {
        "id": "b5C5lbkf2lnJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41kQ030UylSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f3ecd8f-d24e-4861-f1c7-eee658a52a9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will extract the data for each model"
      ],
      "metadata": {
        "id": "ouklXMOZ3804"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# path to the zip file\n",
        "zip_file_path = 'drive/MyDrive/Proiect DeepLearning/DeepFMI_local_data.zip'\n",
        "\n",
        "# the paths to the datasets within the zip file\n",
        "dataset_paths = [\n",
        "    'FMI_local_data/celebhq_real_data',\n",
        "    'FMI_local_data/lama',\n",
        "    'FMI_local_data/ldm',\n",
        "    'FMI_local_data/pluralistic',\n",
        "    'FMI_local_data/repaint'\n",
        "]\n",
        "\n",
        "# create a ZipFile object\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Iterate through the dataset paths\n",
        "    for dataset_path in dataset_paths:\n",
        "         zip_ref.extractall(members=[\n",
        "            name for name in zip_ref.namelist()\n",
        "            if name.startswith(dataset_path)\n",
        "        ], path='/content/')  # Extract to the '/content/' directory\n"
      ],
      "metadata": {
        "id": "n-DmN-2q1CKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Models"
      ],
      "metadata": {
        "id": "NXiICjB2YboU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# important libraries\n",
        "import torch\n",
        "import glob\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import timm\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "from sklearn.metrics import average_precision_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "from tqdm import tqdm  # for progress bar"
      ],
      "metadata": {
        "id": "vTw5sMoNYkQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepFakeDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Takes two folders (real vs fake) and assigns labels 0 / 1.\n",
        "    \"\"\"\n",
        "    def __init__(self, real_folder: str, fake_folder: str, transform=None):\n",
        "        # grab all .png under each\n",
        "        self.real_paths = sorted(glob.glob(os.path.join(real_folder, '*.png')))\n",
        "        self.fake_paths = sorted(glob.glob(os.path.join(fake_folder, '*.png')))\n",
        "\n",
        "        # create a single list of (path, label)\n",
        "        # real = 0, fake = 1\n",
        "        self.samples = (\n",
        "            [(p, 0) for p in self.real_paths] +\n",
        "            [(p, 1) for p in self.fake_paths]\n",
        "        )\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label"
      ],
      "metadata": {
        "id": "KAWUXq05EQeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model_dataloaders(\n",
        "    root_dir: str,            # contains subfolders: lama/, ldm/, repaint/, pluralistic/\n",
        "    real_root: str,           # path to celebhq_real_data\n",
        "    model_names: list[str],   # ['lama','ldm','repaint','pluralistic']\n",
        "    splits: list[str] = ('train','valid','test'),\n",
        "    batch_size: int = 16,\n",
        "    img_size: int = 256,\n",
        "    num_workers: int = 2\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns a dict:\n",
        "      { model_name: { split: DataLoader, … }, … }\n",
        "      Each loader mixes real vs that model's fake images.\n",
        "    \"\"\"\n",
        "\n",
        "    # strong augmentation for train\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(img_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914,0.4822,0.4465), (0.2023,0.1994,0.2010))\n",
        "    ])\n",
        "\n",
        "    # weak augmentation for val/test\n",
        "    test_tf = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914,0.4822,0.4465), (0.2023,0.1994,0.2010))\n",
        "    ])\n",
        "\n",
        "    dataloaders = {}\n",
        "    for model_name in model_names:\n",
        "            dataloaders[model_name] = {}\n",
        "            for split in splits:\n",
        "                real_folder = os.path.join(real_root, split)\n",
        "                fake_folder = os.path.join(root_dir, model_name, split)\n",
        "\n",
        "                tf = train_tf if split=='train' else test_tf\n",
        "                ds = DeepFakeDataset(real_folder, fake_folder, transform=tf)\n",
        "\n",
        "                dataloaders[model_name][split] = DataLoader(\n",
        "                    ds,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=(split=='train'),\n",
        "                    num_workers=num_workers,\n",
        "                    pin_memory=True\n",
        "                )\n",
        "\n",
        "    return dataloaders"
      ],
      "metadata": {
        "id": "IQDzGwDLZ7Nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root = \"/content/FMI_local_data\"\n",
        "deepfake_models = [\"lama\", \"ldm\", \"repaint\", \"pluralistic\"]\n",
        "loaders = make_model_dataloaders(\n",
        "    root_dir=root,\n",
        "    real_root=os.path.join(root, \"celebhq_real_data\"),\n",
        "    model_names=deepfake_models,\n",
        "    splits=['train', 'valid', 'test'],\n",
        "    batch_size=16,\n",
        "    img_size=256,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "# test\n",
        "ldm_train_loader = loaders['ldm']['train']\n",
        "print(f\"ldm train batches: {len(ldm_train_loader)}\")"
      ],
      "metadata": {
        "id": "2L-jqcagYEQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03b45555-c1a0-4687-fa53-bc1eba72c60d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ldm train batches: 1125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_timm_scratch_one_epoch(\n",
        "    dataloaders: dict,\n",
        "    model_name: str = 'xception41',\n",
        "    num_classes: int = 2,\n",
        "    lr: float = 1e-3,\n",
        "    device: str = None\n",
        "):\n",
        "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    results = {}\n",
        "\n",
        "    for name, splits in dataloaders.items():\n",
        "        print(f\"\\n=== Training {model_name} from scratch on '{name}' (1 epoch) ===\")\n",
        "\n",
        "        # 1) instantiate model without pretrained weights\n",
        "        model = timm.create_model(model_name, pretrained=False, num_classes=num_classes)\n",
        "        model = model.to(device)\n",
        "\n",
        "        # 2) loss & optimizer\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "        # 3) Single training epoch\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for imgs, labels in splits['train']:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(imgs)             # raw logits\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "        avg_loss = running_loss / len(splits['train'].dataset)\n",
        "        print(f\"  Train 1-epoch loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # 4) Validation\n",
        "        model.eval()\n",
        "        correct = total = 0\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in splits['valid']:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                preds = model(imgs).argmax(dim=1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "        valid_acc = correct / total\n",
        "        print(f\"  Valid accuracy: {valid_acc:.4%}\")\n",
        "\n",
        "\n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'valid_acc': valid_acc,\n",
        "        }\n",
        "\n",
        "        # clear GPU memory\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "Sg9hrsgjbcQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = train_timm_scratch_one_epoch(\n",
        "    dataloaders=loaders,\n",
        "    model_name='xception41',\n",
        "    num_classes=2,\n",
        "    lr=1e-3\n",
        ")"
      ],
      "metadata": {
        "id": "qzz6i8EVcbCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1b0b973-5ac8-4da3-d3fb-14a9df9ede56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training xception41 from scratch on 'lama' (1 epoch) ===\n",
            "  Train 1-epoch loss: 0.7056\n",
            "  Valid accuracy: 59.2778%\n",
            "\n",
            "=== Training xception41 from scratch on 'ldm' (1 epoch) ===\n",
            "  Train 1-epoch loss: 0.7077\n",
            "  Valid accuracy: 50.0556%\n",
            "\n",
            "=== Training xception41 from scratch on 'repaint' (1 epoch) ===\n",
            "  Train 1-epoch loss: 0.7104\n",
            "  Valid accuracy: 50.0000%\n",
            "\n",
            "=== Training xception41 from scratch on 'pluralistic' (1 epoch) ===\n",
            "  Train 1-epoch loss: 0.6863\n",
            "  Valid accuracy: 53.1111%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save models in drive/MyDrive/Proiect DeepLearning/Task-1/First-Method\n",
        "save_path = 'drive/MyDrive/Proiect DeepLearning/Task-1/First-Method/results.pth'\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)  # Ensure the directory exists\n",
        "torch.save(results, save_path)\n",
        "print(f\"Results saved to: {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDuY3UOBeBhu",
        "outputId": "bdcad502-afc8-4e30-edaa-e059ee2330b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to: drive/MyDrive/Proiect DeepLearning/First-Method/results.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cross_table_ap(results: dict, dataloaders: dict, device=None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a cross-table with average precision score (macro-averaged).\n",
        "    Assumes classification with multiple classes.\n",
        "    \"\"\"\n",
        "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    gens = list(results.keys())\n",
        "    table = pd.DataFrame(index=gens, columns=gens, dtype=float)\n",
        "\n",
        "    for train_gen, result_dict in results.items():\n",
        "        model = result_dict['model']\n",
        "        model.to(device).eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for test_gen in gens:\n",
        "                all_probs = []\n",
        "                all_targets = []\n",
        "                for imgs, labels in dataloaders[test_gen]['test']:\n",
        "                    imgs = imgs.to(device)\n",
        "                    logits = model(imgs)  # raw outputs\n",
        "                    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "                    all_probs.append(probs)\n",
        "                    all_targets.append(labels.cpu().numpy())\n",
        "\n",
        "                all_probs = np.concatenate(all_probs, axis=0)\n",
        "                all_targets = np.concatenate(all_targets, axis=0)\n",
        "\n",
        "                # Convert labels to one-hot\n",
        "                num_classes = all_probs.shape[1]\n",
        "                targets_onehot = np.eye(num_classes)[all_targets]\n",
        "\n",
        "                ap_score = average_precision_score(targets_onehot, all_probs, average='macro')\n",
        "                table.loc[test_gen, train_gen] = ap_score\n",
        "\n",
        "    return table"
      ],
      "metadata": {
        "id": "jKmCBExYv-m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.serialization.add_safe_globals([timm.models.xception_aligned.XceptionAligned])\n",
        "# tmp = torch.load('drive/MyDrive/Proiect DeepLearning/Task-1/First-Method/results.pth', weights_only=False)\n",
        "df = build_cross_table_ap(results, loaders)\n",
        "print(\"\\nCross-Generator Accuracy Table:\\n\")\n",
        "display(Markdown(df.to_markdown(floatfmt=\".3f\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "nSCEumV2wNyt",
        "outputId": "c8b4a2b4-eb91-42ca-f9b3-08249508d077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-Generator Accuracy Table:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "|             |   lama |   ldm |   repaint |   pluralistic |\n|:------------|-------:|------:|----------:|--------------:|\n| lama        |  0.641 | 0.493 |     0.536 |         0.517 |\n| ldm         |  0.569 | 0.496 |     0.511 |         0.536 |\n| repaint     |  0.572 | 0.487 |     0.517 |         0.531 |\n| pluralistic |  0.667 | 0.491 |     0.580 |         0.700 |"
          },
          "metadata": {}
        }
      ]
    }
  ]
}