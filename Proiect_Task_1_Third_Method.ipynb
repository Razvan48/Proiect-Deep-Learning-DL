{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo5ZPflht5ue"
      },
      "source": [
        "# Deepfake Image Detection\n",
        "\n",
        "Autori: Bucă Mihnea-Vicențiu; Căpatână Răzvan-Nicolae; Luculescu Teodor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6MNiBTqt6xg"
      },
      "source": [
        "## Cross-generator deepfake detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOk5UWm7t8zw"
      },
      "source": [
        "We want to evaluate the generalization capabilities of deepfake detection methods: how well detectors work when tested on images produced by other generators than those seen at training. For this, we will train on images coming from one generator and test on images coming from other generators. We will compare at least three different methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc6YuKuQt-oc"
      },
      "source": [
        "First method:\n",
        "- We will use an image classification architecture, mainly ResNet, that will be trained from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsQjhKF9uACc"
      },
      "source": [
        "Second method:\n",
        "- The same architecture as above, but this time initialized with pre-trained weights. The weights will be obtained by supervised learning (image classification on ImageNet).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFRQ2GFguBhO"
      },
      "source": [
        "Third method:\n",
        "- Large pretrained self-supervised representations followed by a linear classifier. In this case, only the linear classifier is trained; the representations are extracted with a frozen model. [Ojha et al., (2023)](https://github.com/WisconsinAIVision/UniversalFakeDetect) have used this approach in the context of deepfake detection, but differently from us, they have applied it to general fully-generated images. We will train two models, one using CLIP and one using SAM self-supervised representations and compare the results obtained.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j-TtLc3uF4T"
      },
      "source": [
        "For each method, fill in a table with the average precisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIAIypZLuGsT"
      },
      "source": [
        "In this notebook we will use the **Third method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvjVlarzuJT5"
      },
      "source": [
        "### Data\n",
        "\n",
        "The dataset can be downloaded from [here](https://drive.google.com/file/d/1NfLX9bZtOY8dO_yj3cU7pEHGmqItqjg2/view). It contains real images from the CelebAHQ dataset and locally manipulated images produced by four generators: [LDM](https://github.com/CompVis/latent-diffusion), [Pluralistic](https://github.com/lyndonzheng/Pluralistic-Inpainting), [LAMA](https://github.com/advimman/lama), [Repaint](https://github.com/andreas128/RePaint). You can read more about how this dataset was produced in Section 3.3 of the following paper:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_h9QtYrMt2Nw",
        "outputId": "aa599d1d-faac-4347-f512-b915fbb60954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OJewQQXuN6T"
      },
      "source": [
        "We will extract the data for each model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VotPy2yQuOHG"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# path to the zip file\n",
        "zip_file_path = 'drive/MyDrive/Proiect DeepLearning/DeepFMI_local_data.zip'\n",
        "\n",
        "# the paths to the datasets within the zip file\n",
        "dataset_paths = [\n",
        "    'FMI_local_data/celebhq_real_data',\n",
        "    'FMI_local_data/lama',\n",
        "    'FMI_local_data/ldm',\n",
        "    'FMI_local_data/pluralistic',\n",
        "    'FMI_local_data/repaint'\n",
        "]\n",
        "\n",
        "# create a ZipFile object\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Iterate through the dataset paths\n",
        "    for dataset_path in dataset_paths:\n",
        "         zip_ref.extractall(members=[\n",
        "            name for name in zip_ref.namelist()\n",
        "            if name.startswith(dataset_path)\n",
        "        ], path='/content/')  # Extract to the '/content/' directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goCwL1VTDZll",
        "outputId": "f3861729-aef4-45b1-d4c8-6c60f337e210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-lnwsyrgf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-lnwsyrgf\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=c6225cf2a25d3492dd46df459d5ddc4cfc4d40e5942c1b41e7b62cc2eacff380\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sjj540_6/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'git+https://github.com/facebookresearch/segment-anything.git'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crjX9-rpdN2_",
        "outputId": "6c6da3b3-0ecc-4408-80b6-f6ca0a563869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-4fmauo2h\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-4fmauo2h\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: segment_anything\n",
            "  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segment_anything: filename=segment_anything-1.0-py3-none-any.whl size=36592 sha256=cfe279d934416b730a545e23ebec1b68c82dee594f7dd29ed6b2fb18e5dc6c2a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0ruwubra/wheels/15/d7/bd/05f5f23b7dcbe70cbc6783b06f12143b0cf1a5da5c7b52dcc5\n",
            "Successfully built segment_anything\n",
            "Installing collected packages: segment_anything\n",
            "Successfully installed segment_anything-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFIOj4VPuQi3"
      },
      "source": [
        "## Training Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1zumphkuQt2"
      },
      "outputs": [],
      "source": [
        "# important libraries\n",
        "import os\n",
        "import torch\n",
        "import glob\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import timm\n",
        "import clip\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "from IPython.display import display, Markdown\n",
        "from sklearn.metrics import average_precision_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "from tqdm import tqdm  # for progress bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-y4e088uRh4"
      },
      "outputs": [],
      "source": [
        "class DeepFakeDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Takes two folders (real vs fake) and assigns labels 0 / 1.\n",
        "    \"\"\"\n",
        "    def __init__(self, real_folder: str, fake_folder: str, transform=None):\n",
        "        # grab all .png under each\n",
        "        self.real_paths = sorted(glob.glob(os.path.join(real_folder, '*.png')))\n",
        "        self.fake_paths = sorted(glob.glob(os.path.join(fake_folder, '*.png')))\n",
        "\n",
        "        # create a single list of (path, label)\n",
        "        # real = 0, fake = 1\n",
        "        self.samples = (\n",
        "            [(p, 0) for p in self.real_paths] +\n",
        "            [(p, 1) for p in self.fake_paths]\n",
        "        )\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdyFP2jYuTA7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def make_model_dataloaders(\n",
        "    root_dir: str,            # contains subfolders: lama/, ldm/, repaint/, pluralistic/\n",
        "    real_root: str,           # path to celebhq_real_data\n",
        "    model_names: list[str],   # ['lama','ldm','repaint','pluralistic']\n",
        "    splits: list[str] = ('train','valid','test'),\n",
        "    batch_size: int = 16,\n",
        "    img_size: int = 256,\n",
        "    num_workers: int = 2\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      {\n",
        "        'sam':   { model_name: { split: DataLoader, … }, … },\n",
        "        'clip':  { model_name: { split: DataLoader, … }, … },\n",
        "      }\n",
        "     loaders['sam']['ldm']['train'], loaders['clip']['ldm']['valid']\n",
        "    \"\"\"\n",
        "\n",
        "    # SAM-specific\n",
        "    sam_tf = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914,0.4822,0.4465),\n",
        "                             (0.2023,0.1994,0.2010)),\n",
        "    ])\n",
        "\n",
        "    # CLIP-specific\n",
        "    clip_tf = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4815,0.4578,0.4081),\n",
        "                             (0.2686,0.2613,0.2758)),\n",
        "    ])\n",
        "\n",
        "    # container for three \"modes\"\n",
        "    loaders = {mode: {} for mode in ('base','sam','clip')}\n",
        "\n",
        "    for mode, tf in [('sam', sam_tf), ('clip', clip_tf)]:\n",
        "        for model_name in model_names:\n",
        "            loaders[mode].setdefault(model_name, {})\n",
        "            for split in splits:\n",
        "                real_folder = os.path.join(real_root, split)\n",
        "                fake_folder = os.path.join(root_dir, model_name, split)\n",
        "\n",
        "                # pick transform\n",
        "                if mode == 'sam':\n",
        "                    transform = sam_tf\n",
        "                elif mode == 'clip':\n",
        "                    transform = clip_tf\n",
        "\n",
        "                # create dataset\n",
        "                ds = DeepFakeDataset(real_folder, fake_folder, transform=transform)\n",
        "                loaders[mode][model_name][split] = DataLoader(\n",
        "                    ds,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=(split == 'train'),\n",
        "                    num_workers=num_workers,\n",
        "                    pin_memory=True\n",
        "                )\n",
        "\n",
        "    return loaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Sb085zHuUVZ",
        "outputId": "6c19485f-a02b-4651-e2c9-83af30d3b609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ldm train batches: 1125\n"
          ]
        }
      ],
      "source": [
        "root = \"/content/FMI_local_data\"\n",
        "deepfake_models = [\"lama\", \"ldm\", \"repaint\", \"pluralistic\"]\n",
        "loaders = make_model_dataloaders(\n",
        "    root_dir=root,\n",
        "    real_root=os.path.join(root, \"celebhq_real_data\"),\n",
        "    model_names=deepfake_models,\n",
        "    splits=['train', 'valid', 'test'],\n",
        "    batch_size=16,\n",
        "    img_size=256,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "# test\n",
        "ldm_train_loader = loaders['clip']['ldm']['train']\n",
        "print(f\"ldm train batches: {len(ldm_train_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjQz6KeVwJsY"
      },
      "source": [
        "#### CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9jMBgPBub5a"
      },
      "outputs": [],
      "source": [
        "def train_clip(\n",
        "    dataloaders: dict,\n",
        "    num_classes: int = 2,\n",
        "    num_epochs: int = 5,\n",
        "    lr: float = 1e-3,\n",
        "    device: str = None,\n",
        "    clip_size: int = 224\n",
        "):\n",
        "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
        "    visual = clip_model.visual.eval().float()\n",
        "    for p in visual.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    feat_dim = visual.output_dim\n",
        "    results = {}\n",
        "\n",
        "    for gen, splits in dataloaders.items():\n",
        "        print(f\"\\n=== [CLIP] probe on '{gen}' ===\")\n",
        "        head = nn.Linear(feat_dim, num_classes).to(device)\n",
        "        opt  = optim.Adam(head.parameters(), lr=lr)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        # --- Training ---\n",
        "        for epoch in range(1, num_epochs+1):\n",
        "            head.train()\n",
        "            total_loss = 0.0\n",
        "            count = 0\n",
        "            for imgs, labels in splits['train']:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    feats = visual(imgs).flatten(1)  # (B, feat_dim)\n",
        "                logits = head(feats)\n",
        "                loss = crit(logits, labels)\n",
        "\n",
        "                opt.zero_grad()\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "                total_loss += loss.item() * imgs.size(0)\n",
        "                count += imgs.size(0)\n",
        "\n",
        "            avg_loss = total_loss / count\n",
        "            print(f\"  Epoch {epoch}/{num_epochs} — head loss: {avg_loss:.4f}\")\n",
        "\n",
        "            # --- Validation ---\n",
        "            head.eval()\n",
        "            all_preds, all_labels = [], []\n",
        "            with torch.no_grad():\n",
        "                for imgs, labels in splits['valid']:\n",
        "                    imgs = imgs.to(device)\n",
        "                    feats = visual(imgs).flatten(1)\n",
        "                    preds = head(feats).argmax(1).cpu().tolist()\n",
        "\n",
        "                    all_preds.extend(preds)\n",
        "                    all_labels.extend(labels.tolist())\n",
        "\n",
        "            valid_acc = accuracy_score(all_labels, all_preds)\n",
        "            print(f\"  ▶ Validation accuracy: {valid_acc:.2%}\")\n",
        "\n",
        "            results[gen] = {'head': head, 'valid_acc': valid_acc}\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONpfILHT6S_L",
        "outputId": "3c6eecb9-a879-4b5c-8ade-6dda73c69b33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== [CLIP] probe on 'lama' ===\n",
            "  Epoch 1/5 — head loss: 0.2594\n",
            "  Epoch 2/5 — head loss: 0.0954\n",
            "  Epoch 3/5 — head loss: 0.0605\n",
            "  Epoch 4/5 — head loss: 0.0444\n",
            "  Epoch 5/5 — head loss: 0.0350\n",
            "  ▶ Validation accuracy: 99.28%\n",
            "\n",
            "=== [CLIP] probe on 'ldm' ===\n",
            "  Epoch 1/5 — head loss: 0.2772\n",
            "  Epoch 2/5 — head loss: 0.1146\n",
            "  Epoch 3/5 — head loss: 0.0793\n",
            "  Epoch 4/5 — head loss: 0.0624\n",
            "  Epoch 5/5 — head loss: 0.0525\n",
            "  ▶ Validation accuracy: 98.11%\n",
            "\n",
            "=== [CLIP] probe on 'repaint' ===\n",
            "  Epoch 1/5 — head loss: 0.6395\n",
            "  Epoch 2/5 — head loss: 0.6145\n",
            "  Epoch 3/5 — head loss: 0.6081\n",
            "  Epoch 4/5 — head loss: 0.6022\n",
            "  Epoch 5/5 — head loss: 0.5990\n",
            "  ▶ Validation accuracy: 65.00%\n",
            "\n",
            "=== [CLIP] probe on 'pluralistic' ===\n",
            "  Epoch 1/5 — head loss: 0.3261\n",
            "  Epoch 2/5 — head loss: 0.1986\n",
            "  Epoch 3/5 — head loss: 0.1678\n",
            "  Epoch 4/5 — head loss: 0.1520\n",
            "  Epoch 5/5 — head loss: 0.1426\n",
            "  ▶ Validation accuracy: 94.22%\n"
          ]
        }
      ],
      "source": [
        "clip_results = train_clip(loaders['clip'], num_classes=2, num_epochs=5, lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = 'drive/MyDrive/Proiect DeepLearning/Task-1/Third-Method/clip_results.pth'\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)  # Ensure the directory exists\n",
        "torch.save(clip_results, save_path)\n",
        "print(f\"Results saved to: {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fgoLRp-fy3d",
        "outputId": "8baa6e8f-a0f9-4307-f206-cc61c614df4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to: drive/MyDrive/Proiect DeepLearning/Third-Method/clip_results.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cross_table_clip(\n",
        "    results: dict,\n",
        "    dataloaders: dict,\n",
        "    device: str = None,\n",
        "    clip_size: int = 224\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cross‐table of macro‐AP for CLIP+linear‐head classifiers.\n",
        "    Expects `results[gen]['head']` to be the trained nn.Linear,\n",
        "    and `dataloaders[gen]['test']` to yield (B,3,224,224),label.\n",
        "    \"\"\"\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # load and freeze CLIP visual encoder\n",
        "    clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
        "    visual = clip_model.visual.eval().float()\n",
        "    for p in visual.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    gens = list(results.keys())\n",
        "    table = pd.DataFrame(index=gens, columns=gens, dtype=float)\n",
        "\n",
        "    for train_gen, result_dict in results.items():\n",
        "        head = result_dict[\"head\"].to(device).eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for test_gen in gens:\n",
        "                all_probs = []\n",
        "                all_targets = []\n",
        "\n",
        "                for imgs, labels in dataloaders[test_gen][\"test\"]:\n",
        "                    imgs = imgs.to(device)\n",
        "                    feats = visual(imgs).flatten(1)  # (B, feat_dim)\n",
        "                    logits = head(feats)\n",
        "                    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "                    all_probs.append(probs)\n",
        "                    all_targets.append(labels.numpy())\n",
        "\n",
        "                all_probs = np.concatenate(all_probs, axis=0)\n",
        "                all_targets = np.concatenate(all_targets, axis=0)\n",
        "                onehot   = np.eye(all_probs.shape[1])[all_targets]\n",
        "\n",
        "                ap = average_precision_score(onehot, all_probs, average=\"macro\")\n",
        "                table.loc[test_gen, train_gen] = ap\n",
        "\n",
        "    return table"
      ],
      "metadata": {
        "id": "ge9S2jTbGhVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = build_cross_table_clip(clip_results, loaders['clip'])\n",
        "print(\"\\nCross-Generator Accuracy Table:\\n\")\n",
        "display(Markdown(df.to_markdown(floatfmt=\".3f\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "4auFX2__GTF3",
        "outputId": "0a32a6c3-e220-4847-900f-3ac2710cf029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-Generator Accuracy Table:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "|             |   lama |   ldm |   repaint |   pluralistic |\n|:------------|-------:|------:|----------:|--------------:|\n| lama        |  1.000 | 0.535 |     0.522 |         0.675 |\n| ldm         |  0.600 | 1.000 |     0.901 |         0.977 |\n| repaint     |  0.511 | 0.620 |     0.723 |         0.615 |\n| pluralistic |  0.765 | 0.951 |     0.880 |         0.992 |"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doZHAIK3x5YV"
      },
      "source": [
        "#### SAM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SamClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A classifier that uses a frozen SAM image encoder backbone\n",
        "    and a trainable linear head for classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, image_encoder: nn.Module, num_classes: int):\n",
        "        super().__init__()\n",
        "        # Use the SAM image encoder as backbone (frozen elsewhere)\n",
        "        self.backbone = image_encoder\n",
        "        # Linear head: assume backbone outputs 256 channels after pooling\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get image embeddings from SAM (shape: [B, 256, H, W])\n",
        "        embeddings = self.backbone(x)\n",
        "        # Global average pool over spatial dims -> [B, 256]\n",
        "        pooled = embeddings.mean(dim=(2, 3))\n",
        "        # Linear classification head\n",
        "        return self.classifier(pooled)"
      ],
      "metadata": {
        "id": "VnORAgv2-KcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtWohl3nx5uP"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_sam(dataloaders: dict, num_classes: int, num_epochs: int,\n",
        "              lr: float, accumulation_steps: int,\n",
        "              weight_decay: float, device: torch.device):\n",
        "    results = {}\n",
        "\n",
        "    # Load SAM ViT-B backbone with pretrained checkpoint\n",
        "    # (Assumes checkpoint downloaded; see segment-anything README for download links&#8203;:contentReference[oaicite:3]{index=3}.)\n",
        "    sam = sam_model_registry[\"vit_b\"](checkpoint=\"/content/sam_vit_b_01ec64.pth\")\n",
        "    sam.eval()\n",
        "\n",
        "    # Adapt backbone for 256x256 inputs\n",
        "    # - Set image encoder expected size\n",
        "    sam.image_encoder.img_size = 256\n",
        "\n",
        "    # - Compute new grid dimensions\n",
        "    patch_size = sam.image_encoder.patch_embed.proj.stride[0]  # typically 16\n",
        "    new_grid = 256 // patch_size\n",
        "\n",
        "    if getattr(sam.image_encoder, \"pos_embed\", None) is not None:\n",
        "        pos_embed = sam.image_encoder.pos_embed  # shape [1, H_old, W_old, C]\n",
        "        H_old, W_old = pos_embed.shape[1], pos_embed.shape[2]\n",
        "        if (H_old != new_grid) or (W_old != new_grid):\n",
        "            # Interpolate to new size\n",
        "            pe = pos_embed.data.reshape(1, H_old, W_old, -1).permute(0, 3, 1, 2)  # -> [1, C, H_old, W_old]\n",
        "            pe = F.interpolate(pe, size=(new_grid, new_grid), mode='bilinear', align_corners=False)\n",
        "            pe = pe.permute(0, 2, 3, 1)  # -> [1, H_new, W_new, C]\n",
        "            sam.image_encoder.pos_embed = nn.Parameter(pe)\n",
        "\n",
        "    # Freeze SAM backbone parameters\n",
        "    for param in sam.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # For each dataset, train a new linear head\n",
        "    for name, loaders in dataloaders.items():\n",
        "        print(f\"\\n=== [SAM] probe on '{name}' ===\")\n",
        "        train_loader = loaders[\"train\"]\n",
        "        valid_loader = loaders[\"valid\"]\n",
        "\n",
        "        # Initialize model (with frozen SAM image encoder)\n",
        "        model = SamClassifier(sam.image_encoder, num_classes).to(device)\n",
        "        model.backbone = sam.image_encoder  # reference frozen backbone\n",
        "        # Ensure backbone stays frozen\n",
        "        for param in model.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Loss and optimizer (only classifier parameters are trainable)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = AdamW(model.classifier.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(1, num_epochs + 1):\n",
        "            model.train()\n",
        "            total_loss = 0.0\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            for i, (images, labels) in enumerate(train_loader, start=1):\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_loss += loss.item()\n",
        "                (loss / accumulation_steps).backward()\n",
        "\n",
        "                # Gradient accumulation step\n",
        "                if (i % accumulation_steps == 0) or (i == len(train_loader)):\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "            avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "            # Validation pass\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            with torch.no_grad():\n",
        "                for images, labels in valid_loader:\n",
        "                    images = images.to(device)\n",
        "                    labels = labels.to(device)\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "                    preds = outputs.argmax(dim=1)\n",
        "                    correct += (preds == labels).sum().item()\n",
        "                    total += labels.size(0)\n",
        "\n",
        "            avg_val_loss = val_loss / len(valid_loader)\n",
        "            val_acc = correct / total\n",
        "\n",
        "            print(f\"[{name}] Epoch {epoch}/{num_epochs} | \"\n",
        "                  f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Save the trained model and final validation accuracy\n",
        "        results[name] = (model, val_acc)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgZf_7qlPTD_",
        "outputId": "830c3fca-18fb-457b-ee65-0b7768111d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== [SAM] probe on 'lama' ===\n",
            "[lama] Epoch 1/1 | Train Loss: 0.6380 | Val Loss: 0.5950\n",
            "\n",
            "=== [SAM] probe on 'ldm' ===\n",
            "[ldm] Epoch 1/1 | Train Loss: 0.6803 | Val Loss: 0.6649\n",
            "\n",
            "=== [SAM] probe on 'repaint' ===\n",
            "[repaint] Epoch 1/1 | Train Loss: 0.6908 | Val Loss: 0.6904\n",
            "\n",
            "=== [SAM] probe on 'pluralistic' ===\n",
            "[pluralistic] Epoch 1/1 | Train Loss: 0.6805 | Val Loss: 0.6723\n"
          ]
        }
      ],
      "source": [
        "sam_results = train_sam(\n",
        "    dataloaders=loaders['sam'], # Pass the inner dict {\"train\":..., \"val\":...}\n",
        "    num_classes=2,\n",
        "    num_epochs=1,\n",
        "    lr=1e-3,\n",
        "    weight_decay=0.01,\n",
        "    accumulation_steps=1,\n",
        "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = 'drive/MyDrive/Proiect DeepLearning/Task-1/Third-Method/sam__results.pth'\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)  # Ensure the directory exists\n",
        "torch.save(sam_results, save_path)\n",
        "print(f\"Results saved to: {save_path}\")"
      ],
      "metadata": {
        "id": "MTDu0hp3ibd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dd3ccf1-4ca2-4c31-d099-62ab6b98a886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to: drive/MyDrive/Proiect DeepLearning/Third-Method/sam__results.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cross_table_sam(\n",
        "    results: dict,\n",
        "    dataloaders: dict,\n",
        "    device: str = None,\n",
        "    checkpoint: str = \"/content/sam_vit_b_01ec64.pth\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cross‐table of macro‐AP for SAM+linear‐head classifiers.\n",
        "    \"\"\"\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 1) Load & freeze SAM image encoder, adapt to 256×256\n",
        "    sam = sam_model_registry[\"vit_b\"](checkpoint=checkpoint).to(device).eval()\n",
        "    for p in sam.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # Adapt SAM to 256×256 inputs\n",
        "    sam.image_encoder.img_size = 256\n",
        "    patch_size = sam.image_encoder.patch_embed.proj.stride[0]\n",
        "    grid = 256 // patch_size\n",
        "    # Resize positional embeddings if present\n",
        "    if hasattr(sam.image_encoder, \"pos_embed\"):\n",
        "        pe = sam.image_encoder.pos_embed          # [1, H_old, W_old, C]\n",
        "        H_old, W_old, C = pe.shape[1], pe.shape[2], pe.shape[3]\n",
        "        pe = pe.reshape(1, H_old, W_old, C).permute(0, 3, 1, 2)  # [1,C,H_old,W_old]\n",
        "        pe = F.interpolate(pe, size=(grid, grid), mode=\"bicubic\", align_corners=False)\n",
        "        pe = pe.permute(0, 2, 3, 1).reshape(1, grid, grid, C)\n",
        "        sam.image_encoder.pos_embed = nn.Parameter(pe)\n",
        "\n",
        "    gens = list(results.keys())\n",
        "    table = pd.DataFrame(index=gens, columns=gens, dtype=float)\n",
        "\n",
        "    # 2) Cross‐evaluate\n",
        "    for train_gen, result_dict in results.items():\n",
        "        # grab the trained linear head\n",
        "        # Accessing the model using index 0 of the tuple\n",
        "        head: nn.Linear = result_dict[0].classifier.to(device).eval() # Access the classifier instead of the model\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for test_gen in gens:\n",
        "                all_probs = []\n",
        "                all_targets = []\n",
        "\n",
        "                for imgs, labels in dataloaders[test_gen][\"test\"]:\n",
        "                    imgs = imgs.to(device)\n",
        "                    labels = labels.cpu().numpy()\n",
        "\n",
        "                    # 2a) SAM feature extraction\n",
        "                    feats = sam.image_encoder(imgs)      # [B, C, H, W]\n",
        "                    pooled = feats.mean(dim=(2, 3))      # [B, C]\n",
        "\n",
        "                    # 2b) classification\n",
        "                    logits = head(pooled)                # [B, num_classes]\n",
        "                    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "                    all_probs.append(probs)\n",
        "                    all_targets.append(labels)\n",
        "\n",
        "                all_probs = np.concatenate(all_probs, axis=0)\n",
        "                all_targets = np.concatenate(all_targets, axis=0)\n",
        "                onehot = np.eye(all_probs.shape[1])[all_targets]\n",
        "\n",
        "                ap = average_precision_score(onehot, all_probs, average=\"macro\")\n",
        "                table.loc[train_gen, test_gen] = ap # fixed the order of train_gen and test_gen\n",
        "\n",
        "    return table"
      ],
      "metadata": {
        "id": "zSyLgtapBFMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = build_cross_table_sam(sam_results, loaders['sam'])\n",
        "print(\"\\nCross-Generator Accuracy Table:\\n\")\n",
        "display(Markdown(df.to_markdown(floatfmt=\".3f\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "T7sw76YlB8Y7",
        "outputId": "19c46c53-9608-454d-cf6c-ce288693c678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-Generator Accuracy Table:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "|             |   lama |   ldm |   repaint |   pluralistic |\n|:------------|-------:|------:|----------:|--------------:|\n| lama        |  0.870 | 0.420 |     0.514 |         0.588 |\n| ldm         |  0.349 | 0.741 |     0.552 |         0.561 |\n| repaint     |  0.536 | 0.543 |     0.547 |         0.575 |\n| pluralistic |  0.647 | 0.527 |     0.543 |         0.627 |"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}