{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deepfake Image Detection\n",
        "\n",
        "Autori: Bucă Mihnea-Vicențiu; Căpatână Răzvan-Nicolae; Luculescu Teodor\n"
      ],
      "metadata": {
        "id": "1_8mwALBPLrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model attribution"
      ],
      "metadata": {
        "id": "q9ZmmwsiPPM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part we investigate whether we can identify the generative model that has produced a particular image. We formulate this task as a multiclass classification task, where the input is an image and the output is one of the five classes: “ldm”, “lama”, “pluralistic”, “repaint”, “real”. Experiment with the same methods as for the first task. Report the overall accuracy and the per class accuracy. Display a TSNE plot of the features color coded by the five classes."
      ],
      "metadata": {
        "id": "gXHKRCd9PReK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "\n",
        "The dataset can be downloaded from [here](https://drive.google.com/file/d/1NfLX9bZtOY8dO_yj3cU7pEHGmqItqjg2/view). It contains real images from the CelebAHQ dataset and locally manipulated images produced by four generators: [LDM](https://github.com/CompVis/latent-diffusion), [Pluralistic](https://github.com/lyndonzheng/Pluralistic-Inpainting), [LAMA](https://github.com/advimman/lama), [Repaint](https://github.com/andreas128/RePaint). You can read more about how this dataset was produced in Section 3.3 of the following paper:"
      ],
      "metadata": {
        "id": "ZQQOVQ3pPXSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3qvDqfrxPZNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will extract the data for each model"
      ],
      "metadata": {
        "id": "0snjUdQZPo0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# path to the zip file\n",
        "zip_file_path = 'drive/MyDrive/Proiect DeepLearning/DeepFMI_local_data.zip'\n",
        "\n",
        "# the paths to the datasets within the zip file\n",
        "dataset_paths = [\n",
        "    'FMI_local_data/celebhq_real_data',\n",
        "    'FMI_local_data/lama',\n",
        "    'FMI_local_data/ldm',\n",
        "    'FMI_local_data/pluralistic',\n",
        "    'FMI_local_data/repaint'\n",
        "]\n",
        "\n",
        "# create a ZipFile object\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Iterate through the dataset paths\n",
        "    for dataset_path in dataset_paths:\n",
        "         zip_ref.extractall(members=[\n",
        "            name for name in zip_ref.namelist()\n",
        "            if name.startswith(dataset_path)\n",
        "        ], path='/content/')  # Extract to the '/content/' directory\n"
      ],
      "metadata": {
        "id": "Qq3jK0X3PqoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Models"
      ],
      "metadata": {
        "id": "FarmLgloQFg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# important libraries\n",
        "import torch\n",
        "import glob\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import timm\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "from sklearn.metrics import average_precision_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "from PIL import Image\n",
        "from tqdm import tqdm  # for progress bar\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "2U4S5rcMQD7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepFakeDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Takes a folder of real images and a list of folders of fake images and assigns labels (0 for real, 1/2/3/4 for each fake image).\n",
        "    0 = real\n",
        "    1 = lama\n",
        "    2 = ldm\n",
        "    3 = repaint\n",
        "    4 = pluralistic\n",
        "    \"\"\"\n",
        "    def __init__(self, real_folder: str, fake_folders: list[str], transform=None):\n",
        "        # grab all .png under each\n",
        "        self.real_paths = sorted(glob.glob(os.path.join(real_folder, '*.png')))\n",
        "        self.samples = [(p, 0) for p in self.real_paths]\n",
        "\n",
        "        self.fake_paths = []\n",
        "        for label, fake_folder in enumerate(fake_folders):\n",
        "            current_fake_paths = sorted(glob.glob(os.path.join(fake_folder, '*.png')))\n",
        "            self.fake_paths += current_fake_paths\n",
        "            self.samples += [(p, label + 1) for p in current_fake_paths]\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label"
      ],
      "metadata": {
        "id": "fRTMXIRzQOg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model_dataloaders(\n",
        "    root_dir: str,            # contains subfolders: lama/, ldm/, repaint/, pluralistic/\n",
        "    real_root: str,           # path to celebhq_real_data\n",
        "    model_names: list[str],   # ['lama','ldm','repaint','pluralistic']\n",
        "    splits: list[str] = ('train','valid','test'),\n",
        "    batch_size: int = 16,\n",
        "    img_size: int = 256,\n",
        "    num_workers: int = 2\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns a dict:\n",
        "      { split: DataLoader, … }\n",
        "      Each loader mixes real vs models' fake images.\n",
        "    \"\"\"\n",
        "\n",
        "    # strong augmentation for train\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(img_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914,0.4822,0.4465), (0.2023,0.1994,0.2010))\n",
        "    ])\n",
        "\n",
        "    # weak augmentation for val/test\n",
        "    test_tf = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914,0.4822,0.4465), (0.2023,0.1994,0.2010))\n",
        "    ])\n",
        "\n",
        "    dataloaders = {}\n",
        "    for split in splits:\n",
        "        real_folder = os.path.join(real_root, split)\n",
        "        fake_folders = [os.path.join(root_dir, model_name, split) for model_name in model_names]\n",
        "\n",
        "        tf = train_tf if split=='train' else test_tf\n",
        "        ds = DeepFakeDataset(real_folder, fake_folders, transform=tf)\n",
        "\n",
        "        dataloaders[split] = DataLoader(\n",
        "            ds,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=(split=='train'),\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "    return dataloaders"
      ],
      "metadata": {
        "id": "7nG6KgObSEyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root = \"/content/FMI_local_data\"\n",
        "deepfake_models = [\"lama\", \"ldm\", \"repaint\", \"pluralistic\"]\n",
        "loaders = make_model_dataloaders(\n",
        "    root_dir=root,\n",
        "    real_root=os.path.join(root, \"celebhq_real_data\"),\n",
        "    model_names=deepfake_models,\n",
        "    splits=['train', 'valid', 'test'],\n",
        "    batch_size=16,\n",
        "    img_size=256,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "# test\n",
        "train_loader = loaders['train']\n",
        "print(f\"train batches: {len(train_loader)}\")"
      ],
      "metadata": {
        "id": "3rnyE-_JTQyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_timm(\n",
        "    dataloaders,         # dict: { name: {'train','valid','test'} DataLoaders }\n",
        "    num_classes: int = 5,\n",
        "    num_epochs: int = 2,\n",
        "    lr_head: float = 1e-3,\n",
        "    lr_ft: float = 1e-4,\n",
        "    weight_decay: float = 1e-5,\n",
        "    freeze_epochs: int = 5,\n",
        "    device: str = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Fine-tune Xception41 pretrained on ImageNet-1K (supervised)\n",
        "    - Phase 1: freeze backbone, train only the new classifier head\n",
        "    - Phase 2: unfreeze entire network and fine-tune\n",
        "    \"\"\"\n",
        "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    results = {}\n",
        "\n",
        "    label_to_model = {\n",
        "        0: 'real',\n",
        "        1: 'lama',\n",
        "        2: 'ldm',\n",
        "        3: 'repaint',\n",
        "        4: 'pluralistic'\n",
        "    }\n",
        "\n",
        "    print(f\"\\n=== [Xception-Imagenet] Training ===\")\n",
        "\n",
        "    # 1) Load Xception41 with supervised ImageNet‐1K weights\n",
        "    model = timm.create_model(\n",
        "        'xception41',\n",
        "        pretrained='imagenet',\n",
        "        num_classes=num_classes\n",
        "    ).to(device)\n",
        "\n",
        "    # 2) Phase 1: freeze all layers except the classifier head\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "    # timm’s get_classifier() returns the final Linear\n",
        "    for p in model.get_classifier().parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    optimizer = optim.Adam(\n",
        "        model.get_classifier().parameters(),\n",
        "        lr=lr_head,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    scheduler = optim.lr_scheduler.StepLR(\n",
        "        optimizer, step_size=freeze_epochs, gamma=0.1\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Head-only training\n",
        "    head_only_train_label_cnts = []\n",
        "    head_only_train_accs = []\n",
        "    head_only_all_train_accs = []\n",
        "\n",
        "    for epoch in range(1, freeze_epochs + 1):\n",
        "\n",
        "        head_only_train_label_cnts.append([0] * num_classes)\n",
        "        head_only_train_accs.append([0] * num_classes)\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for imgs, labels in dataloaders['train']:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(imgs)             # raw logits\n",
        "            loss = criterion(logits, labels) # CE on logits\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "            preds = logits.argmax(dim=1)\n",
        "            for pred, label in zip(preds, labels):\n",
        "                head_only_train_label_cnts[-1][label] += 1\n",
        "                if pred == label:\n",
        "                    head_only_train_accs[-1][label] += 1\n",
        "\n",
        "        scheduler.step()\n",
        "        avg = running_loss / len(dataloaders['train'].dataset)\n",
        "        print(f\" [Head] Epoch {epoch}/{freeze_epochs} — loss: {avg:.4f}\")\n",
        "\n",
        "        train_acc = sum(head_only_train_accs[-1]) / sum(head_only_train_label_cnts[-1])\n",
        "        head_only_all_train_accs.append(train_acc)\n",
        "        print(f\" ▶ train acc {train_acc:.4%}\")\n",
        "\n",
        "        for label in range(num_classes):\n",
        "            if head_only_train_label_cnts[-1][label] != 0:\n",
        "              train_acc = head_only_train_accs[-1][label] / head_only_train_label_cnts[-1][label]\n",
        "            else:\n",
        "              train_acc = 0\n",
        "            print(f\"   ▶ train acc {label_to_model[label]} {train_acc:.4%}\")\n",
        "\n",
        "    # 3) Phase 2: unfreeze everything, fine-tune\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=lr_ft,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    scheduler = optim.lr_scheduler.StepLR(\n",
        "        optimizer, step_size=5, gamma=0.1\n",
        "    )\n",
        "\n",
        "    fine_tune_train_label_cnts = []\n",
        "    fine_tune_train_accs = []\n",
        "    fine_tune_all_train_accs = []\n",
        "\n",
        "    for epoch in range(freeze_epochs + 1, num_epochs + 1):\n",
        "\n",
        "        fine_tune_train_label_cnts.append([0] * num_classes)\n",
        "        fine_tune_train_accs.append([0] * num_classes)\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for imgs, labels in dataloaders['train']:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "            preds = logits.argmax(dim=1)\n",
        "            for pred, label in zip(preds, labels):\n",
        "                fine_tune_train_label_cnts[-1][label] += 1\n",
        "                if pred == label:\n",
        "                    fine_tune_train_accs[-1][label] += 1\n",
        "\n",
        "        scheduler.step()\n",
        "        avg = running_loss / len(dataloaders['train'].dataset)\n",
        "        print(f\" [Fine-tune] Epoch {epoch}/{num_epochs} — loss: {avg:.4f}\")\n",
        "\n",
        "        train_acc = sum(fine_tune_train_accs[-1]) / sum(fine_tune_train_label_cnts[-1])\n",
        "        fine_tune_all_train_accs.append(train_acc)\n",
        "        print(f\" ▶ train acc {train_acc:.4%}\")\n",
        "\n",
        "        for label in range(num_classes):\n",
        "            if fine_tune_train_label_cnts[-1][label] != 0:\n",
        "              train_acc = fine_tune_train_accs[-1][label] / fine_tune_train_label_cnts[-1][label]\n",
        "            else:\n",
        "              train_acc = 0\n",
        "            print(f\"   ▶ train acc {label_to_model[label]} {train_acc:.4%}\")\n",
        "\n",
        "    # 4) Validation\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "\n",
        "    valid_label_cnts = [0] * num_classes\n",
        "    valid_accs = [0] * num_classes\n",
        "\n",
        "    valid_features = []\n",
        "    valid_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in dataloaders['valid']:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            preds = model(imgs).argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            for pred, label in zip(preds, labels):\n",
        "                valid_label_cnts[label] += 1\n",
        "                if pred == label:\n",
        "                    valid_accs[label] += 1\n",
        "\n",
        "            feats = model.forward_features(imgs)\n",
        "            valid_features.append(feats.cpu())\n",
        "            valid_labels.append(labels.cpu())\n",
        "\n",
        "    valid_features = torch.cat(valid_features, dim=0).numpy() # from many tensors for each batch to just one\n",
        "    valid_labels = torch.cat(valid_labels, dim=0).numpy()\n",
        "\n",
        "    valid_acc = correct / total\n",
        "    print(f\" ▶ valid acc {valid_acc:.4%}\")\n",
        "\n",
        "    for label in range(num_classes):\n",
        "        if valid_label_cnts[label] != 0:\n",
        "          valid_acc = valid_accs[label] / valid_label_cnts[label]\n",
        "        else:\n",
        "          valid_acc = 0\n",
        "        print(f\"   ▶ valid acc {label_to_model[label]} {valid_acc:.4%}\")\n",
        "\n",
        "\n",
        "    results[name] = {\n",
        "        'model': model,\n",
        "        'valid_acc': valid_acc,\n",
        "\n",
        "        'head_only_all_train_acc': head_only_all_train_accs[-1],\n",
        "        'fine_tune_all_train_acc': fine_tune_all_train_accs[-1],\n",
        "\n",
        "        'valid_features': valid_features,\n",
        "        'valid_labels': valid_labels\n",
        "    }\n",
        "    for label in range(num_classes):\n",
        "      if head_only_train_label_cnts[-1][label] != 0:\n",
        "        results[f'head_only_train_acc_{label_to_model[label]}'] = head_only_train_accs[-1][label] / head_only_train_label_cnts[-1][label]\n",
        "      else:\n",
        "        results[f'head_only_train_acc_{label_to_model[label]}'] = 0\n",
        "      if fine_tune_train_label_cnts[-1][label] != 0:\n",
        "        results[f'fine_tune_train_acc_{label_to_model[label]}'] = fine_tune_train_accs[-1][label] / fine_tune_train_label_cnts[-1][label]\n",
        "      else:\n",
        "        results[f'fine_tune_train_acc_{label_to_model[label]}'] = 0\n",
        "\n",
        "      if valid_label_cnts[label] != 0:\n",
        "        results[f'valid_acc_{label_to_model[label]}'] = valid_accs[label] / valid_label_cnts[label]\n",
        "      else:\n",
        "        results[f'valid_acc_{label_to_model[label]}'] = 0\n",
        "\n",
        "    # clear GPU memory\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "h5OCOoPMTtrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_tsne(features, labels, num_classes=5):\n",
        "  tsne = TSNE(n_components=2, random_state=17, perplexity=30)\n",
        "  tsne_features = tsne.fit_transform(features)\n",
        "\n",
        "  plt.figure(figsize=(10, 8))\n",
        "\n",
        "  label_to_model = {\n",
        "      0: 'real',\n",
        "      1: 'lama',\n",
        "      2: 'ldm',\n",
        "      3: 'repaint',\n",
        "      4: 'pluralistic'\n",
        "  }\n",
        "\n",
        "  label_to_color = {\n",
        "      0: 'green',\n",
        "      1: 'red',\n",
        "      2: 'blue',\n",
        "      3: 'yellow',\n",
        "      4: 'purple'\n",
        "  }\n",
        "\n",
        "  for label in range(num_classes):\n",
        "    mask = labels == label\n",
        "\n",
        "    plt.scatter(\n",
        "        tsne_features[mask, 0],\n",
        "        tsne_features[mask, 1],\n",
        "        color=label_to_color[label],\n",
        "        label=label_to_model[label], alpha=0.5\n",
        "    )\n",
        "\n",
        "  plt.legend()\n",
        "  plt.title('TSNE Plot of Features')\n",
        "  plt.xlabel('1st Dimension')\n",
        "  plt.ylabel('2nd Dimension')\n",
        "  # plt.grid(True)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "vn0-JXy83pfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = train_timm(\n",
        "    dataloaders=loaders,\n",
        "    num_classes = 5,\n",
        "    num_epochs = 2,\n",
        "    lr_head = 1e-3,\n",
        "    lr_ft = 1e-4,\n",
        "    weight_decay = 1e-5,\n",
        "    freeze_epochs = 1,\n",
        ")\n",
        "\n",
        "plot_tsne(results['valid_features'], results['valid_labels'])"
      ],
      "metadata": {
        "id": "ul6_DMKrTiwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# save models in drive/MyDrive/Proiect DeepLearning/Task-2/Second-Method\n",
        "save_path = 'drive/MyDrive/Proiect DeepLearning/Task-2/Second-Method/results.pth'\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)  # Ensure the directory exists\n",
        "torch.save(results, save_path)\n",
        "print(f\"Results saved to: {save_path}\")"
      ],
      "metadata": {
        "id": "520IEn6iTbpv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}