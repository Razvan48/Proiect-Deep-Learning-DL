{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deepfake Image Detection\n",
        "\n",
        "Autori: Bucă Mihnea-Vicențiu; Căpatână Răzvan-Nicolae; Luculescu Teodor\n"
      ],
      "metadata": {
        "id": "1_8mwALBPLrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model attribution"
      ],
      "metadata": {
        "id": "q9ZmmwsiPPM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part we investigate whether we can identify the generative model that has produced a particular image. We formulate this task as a multiclass classification task, where the input is an image and the output is one of the five classes: “ldm”, “lama”, “pluralistic”, “repaint”, “real”. Experiment with the same methods as for the first task. Report the overall accuracy and the per class accuracy. Display a TSNE plot of the features color coded by the five classes."
      ],
      "metadata": {
        "id": "gXHKRCd9PReK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "\n",
        "The dataset can be downloaded from [here](https://drive.google.com/file/d/1NfLX9bZtOY8dO_yj3cU7pEHGmqItqjg2/view). It contains real images from the CelebAHQ dataset and locally manipulated images produced by four generators: [LDM](https://github.com/CompVis/latent-diffusion), [Pluralistic](https://github.com/lyndonzheng/Pluralistic-Inpainting), [LAMA](https://github.com/advimman/lama), [Repaint](https://github.com/andreas128/RePaint). You can read more about how this dataset was produced in Section 3.3 of the following paper:"
      ],
      "metadata": {
        "id": "ZQQOVQ3pPXSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3qvDqfrxPZNx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9eb3b99-05bc-4373-b0aa-b88753beeb54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will extract the data for each model"
      ],
      "metadata": {
        "id": "0snjUdQZPo0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# path to the zip file\n",
        "zip_file_path = 'drive/MyDrive/Proiect DeepLearning/DeepFMI_local_data.zip'\n",
        "\n",
        "# the paths to the datasets within the zip file\n",
        "dataset_paths = [\n",
        "    'FMI_local_data/celebhq_real_data',\n",
        "    'FMI_local_data/lama',\n",
        "    'FMI_local_data/ldm',\n",
        "    'FMI_local_data/pluralistic',\n",
        "    'FMI_local_data/repaint'\n",
        "]\n",
        "\n",
        "# create a ZipFile object\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Iterate through the dataset paths\n",
        "    for dataset_path in dataset_paths:\n",
        "         zip_ref.extractall(members=[\n",
        "            name for name in zip_ref.namelist()\n",
        "            if name.startswith(dataset_path)\n",
        "        ], path='/content/')  # Extract to the '/content/' directory\n"
      ],
      "metadata": {
        "id": "Qq3jK0X3PqoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Models"
      ],
      "metadata": {
        "id": "FarmLgloQFg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# important libraries\n",
        "import torch\n",
        "import glob\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import timm\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "from sklearn.metrics import average_precision_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models\n",
        "# from sam2.build_sam import build_sam2\n",
        "# from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "from PIL import Image\n",
        "from tqdm import tqdm  # for progress bar\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import h5py"
      ],
      "metadata": {
        "id": "2U4S5rcMQD7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepFakeDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Takes a folder of real images and a list of folders of fake images and assigns labels (0 for real, 1/2/3/4 for each fake image).\n",
        "    0 = real\n",
        "    1 = lama\n",
        "    2 = ldm\n",
        "    3 = repaint\n",
        "    4 = pluralistic\n",
        "    \"\"\"\n",
        "    def __init__(self, real_folder: str, fake_folders: list[str], transform=None):\n",
        "        # grab all .png under each\n",
        "        self.real_paths = sorted(glob.glob(os.path.join(real_folder, '*.png')))\n",
        "        self.samples = [(p, 0) for p in self.real_paths]\n",
        "\n",
        "        self.fake_paths = []\n",
        "        for label, fake_folder in enumerate(fake_folders):\n",
        "            current_fake_paths = sorted(glob.glob(os.path.join(fake_folder, '*.png')))\n",
        "            self.fake_paths += current_fake_paths\n",
        "            self.samples += [(p, label + 1) for p in current_fake_paths]\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label"
      ],
      "metadata": {
        "id": "fRTMXIRzQOg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model_dataloaders(\n",
        "    root_dir: str,            # contains subfolders: lama/, ldm/, repaint/, pluralistic/\n",
        "    real_root: str,           # path to celebhq_real_data\n",
        "    model_names: list[str],   # ['lama','ldm','repaint','pluralistic']\n",
        "    splits: list[str] = ('train','valid','test'),\n",
        "    batch_size: int = 16,\n",
        "    img_size: int = 256,\n",
        "    num_workers: int = 2\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns a dict:\n",
        "      { split: DataLoader, … }\n",
        "      Each loader mixes real vs models' fake images.\n",
        "    \"\"\"\n",
        "\n",
        "    # strong augmentation for train\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(img_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914,0.4822,0.4465), (0.2023,0.1994,0.2010))\n",
        "    ])\n",
        "\n",
        "    # weak augmentation for val/test\n",
        "    test_tf = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914,0.4822,0.4465), (0.2023,0.1994,0.2010))\n",
        "    ])\n",
        "\n",
        "    dataloaders = {}\n",
        "    for split in splits:\n",
        "        real_folder = os.path.join(real_root, split)\n",
        "        fake_folders = [os.path.join(root_dir, model_name, split) for model_name in model_names]\n",
        "\n",
        "        tf = train_tf if split=='train' else test_tf\n",
        "        ds = DeepFakeDataset(real_folder, fake_folders, transform=tf)\n",
        "\n",
        "        dataloaders[split] = DataLoader(\n",
        "            ds,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=(split=='train'),\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "    return dataloaders"
      ],
      "metadata": {
        "id": "7nG6KgObSEyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root = \"/content/FMI_local_data\"\n",
        "deepfake_models = [\"lama\", \"ldm\", \"repaint\", \"pluralistic\"]\n",
        "loaders = make_model_dataloaders(\n",
        "    root_dir=root,\n",
        "    real_root=os.path.join(root, \"celebhq_real_data\"),\n",
        "    model_names=deepfake_models,\n",
        "    splits=['train', 'valid', 'test'],\n",
        "    batch_size=16,\n",
        "    img_size=256,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "# test\n",
        "train_loader = loaders['train']\n",
        "print(f\"train batches: {len(train_loader)}\")"
      ],
      "metadata": {
        "id": "3rnyE-_JTQyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07479455-fd47-438c-b242-730b6c08f656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train batches: 2813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "def train_timm(\n",
        "    dataloaders,         # dict: { name: {'train','valid','test'} DataLoaders }\n",
        "    num_classes: int = 5,\n",
        "    num_epochs: int = 2,\n",
        "    lr_head: float = 1e-3,\n",
        "    lr_ft: float = 1e-4,\n",
        "    weight_decay: float = 1e-5,\n",
        "    freeze_epochs: int = 5,\n",
        "    device: str = None,\n",
        "    model = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Fine-tune Xception41 pretrained on ImageNet-1K (supervised)\n",
        "    - Phase 1: freeze backbone, train only the new classifier head\n",
        "    - Phase 2: unfreeze entire network and fine-tune\n",
        "    \"\"\"\n",
        "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    results = {}\n",
        "\n",
        "    label_to_model = {\n",
        "        0: 'real',\n",
        "        1: 'lama',\n",
        "        2: 'ldm',\n",
        "        3: 'repaint',\n",
        "        4: 'pluralistic'\n",
        "    }\n",
        "\n",
        "    print(f\"\\n=== [Xception-Imagenet] Training ===\")\n",
        "\n",
        "    # 1) Load Xception41 with supervised ImageNet‐1K weights\n",
        "    '''\n",
        "    model = timm.create_model(\n",
        "        'xception41',\n",
        "        pretrained='imagenet',\n",
        "        num_classes=num_classes\n",
        "    ).to(device)\n",
        "    '''\n",
        "    model.to(device)\n",
        "\n",
        "    # 2) Phase 1: freeze all layers except the classifier head\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "    # timm’s get_classifier() returns the final Linear\n",
        "    for p in model.get_classifier().parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    optimizer = optim.Adam(\n",
        "        model.get_classifier().parameters(),\n",
        "        lr=lr_head,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    scheduler = optim.lr_scheduler.StepLR(\n",
        "        optimizer, step_size=freeze_epochs, gamma=0.1\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Head-only training\n",
        "    head_only_train_label_cnts = []\n",
        "    head_only_train_accs = []\n",
        "    head_only_all_train_accs = []\n",
        "\n",
        "    for epoch in range(1, freeze_epochs + 1):\n",
        "\n",
        "        head_only_train_label_cnts.append([0] * num_classes)\n",
        "        head_only_train_accs.append([0] * num_classes)\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for imgs, labels in dataloaders['train']:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(imgs)             # raw logits\n",
        "            loss = criterion(logits, labels) # CE on logits\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "            preds = logits.argmax(dim=1)\n",
        "            for pred, label in zip(preds, labels):\n",
        "                head_only_train_label_cnts[-1][label] += 1\n",
        "                if pred == label:\n",
        "                    head_only_train_accs[-1][label] += 1\n",
        "\n",
        "        scheduler.step()\n",
        "        avg = running_loss / len(dataloaders['train'].dataset)\n",
        "        print(f\" [Head] Epoch {epoch}/{freeze_epochs} — loss: {avg:.4f}\")\n",
        "\n",
        "        train_acc = np.sum(np.array(head_only_train_accs[-1])) / np.sum(np.array(head_only_train_label_cnts[-1]))\n",
        "        head_only_all_train_accs.append(train_acc)\n",
        "        print(f\" ▶ train acc {train_acc:.4%}\")\n",
        "\n",
        "        for label in range(num_classes):\n",
        "            if head_only_train_label_cnts[-1][label] != 0:\n",
        "              crt_train_acc = head_only_train_accs[-1][label] / head_only_train_label_cnts[-1][label]\n",
        "            else:\n",
        "              crt_train_acc = 0\n",
        "            print(f\"   ▶ train acc {label_to_model[label]} {crt_train_acc:.4%}\")\n",
        "\n",
        "    # 3) Phase 2: unfreeze everything, fine-tune\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=lr_ft,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    scheduler = optim.lr_scheduler.StepLR(\n",
        "        optimizer, step_size=5, gamma=0.1\n",
        "    )\n",
        "\n",
        "    fine_tune_train_label_cnts = []\n",
        "    fine_tune_train_accs = []\n",
        "    fine_tune_all_train_accs = []\n",
        "\n",
        "    for epoch in range(freeze_epochs + 1, num_epochs + 1):\n",
        "\n",
        "        fine_tune_train_label_cnts.append([0] * num_classes)\n",
        "        fine_tune_train_accs.append([0] * num_classes)\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for imgs, labels in dataloaders['train']:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "            preds = logits.argmax(dim=1)\n",
        "            for pred, label in zip(preds, labels):\n",
        "                fine_tune_train_label_cnts[-1][label] += 1\n",
        "                if pred == label:\n",
        "                    fine_tune_train_accs[-1][label] += 1\n",
        "\n",
        "        scheduler.step()\n",
        "        avg = running_loss / len(dataloaders['train'].dataset)\n",
        "        print(f\" [Fine-tune] Epoch {epoch}/{num_epochs} — loss: {avg:.4f}\")\n",
        "\n",
        "        train_acc = np.sum(np.array(fine_tune_train_accs[-1])) / np.sum(np.array(fine_tune_train_label_cnts[-1]))\n",
        "        fine_tune_all_train_accs.append(train_acc)\n",
        "        print(f\" ▶ train acc {train_acc:.4%}\")\n",
        "\n",
        "        for label in range(num_classes):\n",
        "            if fine_tune_train_label_cnts[-1][label] != 0:\n",
        "              crt_train_acc = fine_tune_train_accs[-1][label] / fine_tune_train_label_cnts[-1][label]\n",
        "            else:\n",
        "              crt_train_acc = 0\n",
        "            print(f\"   ▶ train acc {label_to_model[label]} {crt_train_acc:.4%}\")\n",
        "\n",
        "    # 4) Validation\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "\n",
        "    valid_label_cnts = [0] * num_classes\n",
        "    valid_accs = [0] * num_classes\n",
        "\n",
        "    valid_features = []\n",
        "    valid_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in dataloaders['valid']:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            preds = model(imgs).argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            for pred, label in zip(preds, labels):\n",
        "                valid_label_cnts[label] += 1\n",
        "                if pred == label:\n",
        "                    valid_accs[label] += 1\n",
        "\n",
        "            feats = model.forward_features(imgs)\n",
        "            valid_features.append(feats.detach().cpu())\n",
        "            valid_labels.append(labels.detach().cpu())\n",
        "\n",
        "    valid_features = torch.cat(valid_features, dim=0).numpy() # from many tensors for each batch to just one\n",
        "    valid_labels = torch.cat(valid_labels, dim=0).numpy()\n",
        "\n",
        "    valid_acc = correct / total\n",
        "    print(f\" ▶ valid acc {valid_acc:.4%}\")\n",
        "\n",
        "    for label in range(num_classes):\n",
        "        if valid_label_cnts[label] != 0:\n",
        "          crt_valid_acc = valid_accs[label] / valid_label_cnts[label]\n",
        "        else:\n",
        "          crt_valid_acc = 0\n",
        "        print(f\"   ▶ valid acc {label_to_model[label]} {crt_valid_acc:.4%}\")\n",
        "\n",
        "\n",
        "    results = {\n",
        "        'model': model.state_dict(), # state_dict instead of model\n",
        "        'valid_acc': valid_acc,\n",
        "\n",
        "        'head_only_all_train_acc': head_only_all_train_accs[-1] if len(head_only_all_train_accs) > 0 else 0,\n",
        "        'fine_tune_all_train_acc': fine_tune_all_train_accs[-1] if len(fine_tune_all_train_accs) > 0 else 0,\n",
        "\n",
        "        'valid_features': valid_features,\n",
        "        'valid_labels': valid_labels\n",
        "    }\n",
        "    for label in range(num_classes):\n",
        "      if head_only_train_label_cnts[-1][label] != 0:\n",
        "        results[f'head_only_train_acc_{label_to_model[label]}'] = head_only_train_accs[-1][label] / head_only_train_label_cnts[-1][label]\n",
        "      else:\n",
        "        results[f'head_only_train_acc_{label_to_model[label]}'] = 0\n",
        "      if len(fine_tune_all_train_accs) > 0:\n",
        "        if fine_tune_train_label_cnts[-1][label] != 0:\n",
        "          results[f'fine_tune_train_acc_{label_to_model[label]}'] = fine_tune_train_accs[-1][label] / fine_tune_train_label_cnts[-1][label]\n",
        "        else:\n",
        "          results[f'fine_tune_train_acc_{label_to_model[label]}'] = 0\n",
        "\n",
        "      if valid_label_cnts[label] != 0:\n",
        "        results[f'valid_acc_{label_to_model[label]}'] = valid_accs[label] / valid_label_cnts[label]\n",
        "      else:\n",
        "        results[f'valid_acc_{label_to_model[label]}'] = 0\n",
        "\n",
        "    # clear GPU memory\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "h5OCOoPMTtrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "results = train_timm(\n",
        "    dataloaders=loaders,\n",
        "    num_classes = 5,\n",
        "    num_epochs = 2,\n",
        "    lr_head = 1e-3,\n",
        "    lr_ft = 1e-4,\n",
        "    weight_decay = 1e-5,\n",
        "    freeze_epochs = 1,\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "id": "ul6_DMKrTiwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = 'drive/MyDrive/Proiect DeepLearning/Task-2/Second-Method/results.pth'\n",
        "\n",
        "save_path_dir = 'drive/MyDrive/Proiect DeepLearning/Task-2/Second-Method/'\n",
        "os.makedirs(save_path_dir, exist_ok=True)  # Ensure the directory exists"
      ],
      "metadata": {
        "id": "Uyb2kIW5XyG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions to read/write large data dict from/to storage (memory-efficient)\n",
        "\n",
        "def save_dict_to_hdf5(data, path, model_path):\n",
        "    with h5py.File(path, 'w') as h5file:\n",
        "      for k, v in data.items():\n",
        "        if k == 'model':\n",
        "          torch.save(v, model_path)\n",
        "        else:\n",
        "          _recursively_save(h5file, str(k), v)\n",
        "\n",
        "def _recursively_save(h5group, key, value):\n",
        "    if isinstance(value, dict):\n",
        "        grp = h5group.create_group(key)\n",
        "        for k, v in value.items():\n",
        "            _recursively_save(grp, str(k), v)\n",
        "\n",
        "    elif isinstance(value, (list, tuple)):\n",
        "        grp = h5group.create_group(key)\n",
        "        grp.attrs['__type__'] = 'tuple' if isinstance(value, tuple) else 'list'\n",
        "        for i, item in enumerate(value):\n",
        "            _recursively_save(grp, str(i), item)\n",
        "\n",
        "    elif isinstance(value, torch.Tensor):\n",
        "        dset = h5group.create_dataset(key, data=value.cpu().numpy())\n",
        "        dset.attrs['__type__'] = 'torch_tensor'\n",
        "\n",
        "    elif isinstance(value, np.ndarray):\n",
        "        h5group.create_dataset(key, data=value)\n",
        "\n",
        "    elif isinstance(value, (str, int, float, bool)):\n",
        "        dset = h5group.create_dataset(key, data=value)\n",
        "        dset.attrs['__type__'] = type(value).__name__\n",
        "\n",
        "    elif value is None:\n",
        "        grp = h5group.create_group(key)\n",
        "        grp.attrs['__type__'] = 'none'\n",
        "\n",
        "    else:\n",
        "        raise TypeError(f'Unsupported data type: {type(value)}')\n",
        "\n",
        "\n",
        "def load_dict_from_hdf5(path, model_path):\n",
        "    model_state_dict = torch.load(model_path, map_location='cpu')\n",
        "    results = {}\n",
        "    with h5py.File(path, 'r') as h5file:\n",
        "        results =  _recursively_load(h5file['/'])\n",
        "    results['model'] = model_state_dict\n",
        "\n",
        "    return results\n",
        "\n",
        "def _recursively_load(h5obj):\n",
        "    if isinstance(h5obj, h5py.Dataset):\n",
        "        data = h5obj[()]\n",
        "        dtype = h5obj.attrs.get('__type__')\n",
        "\n",
        "        if dtype == 'torch_tensor':\n",
        "            return torch.tensor(data)\n",
        "        elif dtype == 'int':\n",
        "            return int(data)\n",
        "        elif dtype == 'float':\n",
        "            return float(data)\n",
        "        elif dtype == 'str':\n",
        "            return data.decode() if isinstance(data, bytes) else str(data)\n",
        "        elif dtype == 'bool':\n",
        "            return bool(data)\n",
        "        else:\n",
        "            return data  # numpy array or other\n",
        "\n",
        "    elif isinstance(h5obj, h5py.Group):\n",
        "        dtype = h5obj.attrs.get('__type__')\n",
        "        if dtype == 'list' or dtype == 'tuple':\n",
        "            items = []\n",
        "            for key in sorted(h5obj.keys(), key=lambda x: int(x)):\n",
        "                items.append(_recursively_load(h5obj[key]))\n",
        "            return tuple(items) if dtype == 'tuple' else items\n",
        "        elif dtype == 'none':\n",
        "            return None\n",
        "        else:\n",
        "            return {key: _recursively_load(h5obj[key]) for key in h5obj}\n"
      ],
      "metadata": {
        "id": "zNYGkSjHQ20y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loop_training():\n",
        "  save_path_base = save_path_dir\n",
        "\n",
        "  num_epochs = 10\n",
        "\n",
        "  model_name = 'xception41'\n",
        "  num_classes = 5\n",
        "\n",
        "  model = timm.create_model(\n",
        "        model_name,\n",
        "        pretrained='imagenet',\n",
        "        num_classes=num_classes\n",
        "  )\n",
        "\n",
        "  results = {}\n",
        "\n",
        "  for i in range(1, num_epochs+1):\n",
        "    if i > 1:\n",
        "      results.clear()\n",
        "      results = load_dict_from_hdf5(save_path_base + f'results_{i-1}.pth')\n",
        "      # results = torch.load(save_path_base + f'results_{i-1}.pth')\n",
        "      model.load_state_dict(results['model'])\n",
        "\n",
        "    results = train_timm(\n",
        "        dataloaders=loaders,\n",
        "        num_classes = 5,\n",
        "        num_epochs = 2,\n",
        "        lr_head = 1e-3,\n",
        "        lr_ft = 1e-4,\n",
        "        weight_decay = 1e-5,\n",
        "        freeze_epochs = 1,\n",
        "        model = model\n",
        "    )\n",
        "\n",
        "    save_dict_to_hdf5(results, save_path_base + f'results_{i}.pth')\n",
        "    # torch.save(results, save_path_base + f'results_{i}.pth')\n"
      ],
      "metadata": {
        "id": "DY0Ve5iKQ6Ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loop_training()"
      ],
      "metadata": {
        "id": "CUv0qUVURvGj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "2b4847ac-96fe-4fda-a4b0-449237eb116d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== [Xception-Imagenet] Training ===\n",
            " [Head] Epoch 1/1 — loss: 1.3209\n",
            " ▶ train acc 44.4188%\n",
            "   ▶ train acc real 29.5667%\n",
            "   ▶ train acc lama 70.3778%\n",
            "   ▶ train acc ldm 56.8889%\n",
            "   ▶ train acc repaint 26.9697%\n",
            "   ▶ train acc pluralistic 38.2889%\n",
            " ▶ valid acc 35.5333%\n",
            "   ▶ valid acc real 54.0000%\n",
            "   ▶ valid acc lama 38.2222%\n",
            "   ▶ valid acc ldm 65.3333%\n",
            "   ▶ valid acc repaint 15.0000%\n",
            "   ▶ valid acc pluralistic 5.1111%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-20f4ae7d59d6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloop_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-2947f9bf2c12>\u001b[0m in \u001b[0;36mloop_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     results = train_timm(\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mdataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-a9070dce07a2>\u001b[0m in \u001b[0;36mtrain_timm\u001b[0;34m(dataloaders, num_classes, num_epochs, lr_head, lr_ft, weight_decay, freeze_epochs, device, model)\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'head_only_train_acc_{label_to_model[label]}'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mfine_tune_train_label_cnts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'fine_tune_train_acc_{label_to_model[label]}'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfine_tune_train_accs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfine_tune_train_label_cnts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import os\n",
        "# save models in drive/MyDrive/Proiect DeepLearning/Task-2/Second-Method\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)  # Ensure the directory exists\n",
        "torch.save(results, save_path)\n",
        "print(f\"Results saved to: {save_path}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "520IEn6iTbpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_tsne(features, labels, num_classes=5):\n",
        "  tsne = TSNE(n_components=2, random_state=17, perplexity=30)\n",
        "  tsne_features = tsne.fit_transform(features)\n",
        "\n",
        "  plt.figure(figsize=(10, 8))\n",
        "\n",
        "  label_to_model = {\n",
        "      0: 'real',\n",
        "      1: 'lama',\n",
        "      2: 'ldm',\n",
        "      3: 'repaint',\n",
        "      4: 'pluralistic'\n",
        "  }\n",
        "\n",
        "  label_to_color = {\n",
        "      0: 'green',\n",
        "      1: 'red',\n",
        "      2: 'blue',\n",
        "      3: 'yellow',\n",
        "      4: 'purple'\n",
        "  }\n",
        "\n",
        "  for label in range(num_classes):\n",
        "    mask = labels == label\n",
        "\n",
        "    plt.scatter(\n",
        "        tsne_features[mask, 0],\n",
        "        tsne_features[mask, 1],\n",
        "        color=label_to_color[label],\n",
        "        label=label_to_model[label], alpha=0.5\n",
        "    )\n",
        "\n",
        "  plt.legend()\n",
        "  plt.title('TSNE Plot of Features')\n",
        "  plt.xlabel('1st Dimension')\n",
        "  plt.ylabel('2nd Dimension')\n",
        "  # plt.grid(True)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "vn0-JXy83pfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_idx = 2\n",
        "results = load_dict_from_hdf5(save_path_dir + f'results_{epoch_idx}.pth', save_path_dir + f'model_state_dict_{epoch_idx}.pth')\n"
      ],
      "metadata": {
        "id": "Vvfv5_PTHlGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_tsne(results['valid_features'].mean(axis=(2, 3)), results['valid_labels'])"
      ],
      "metadata": {
        "id": "5bG9sU1GJHd8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}